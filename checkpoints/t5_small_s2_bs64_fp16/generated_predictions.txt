, unless the model is given access to oracle parameters that grow superpolynomially in sequence length. Unlike standard autogressive language models, they perform only polynomial-time computation to compute the probability of the next symbol. Unlike standard autogressive language models, they cannot model distributions whose next-symbol probability is hard to compute.
,, a novel fine-tuning approach that reformulates the tuples of the datasets as sentences. FeSTE is able to generate high quality features and significantly outperform existing fine-tuning solutions.
and reward scaling., we revisit these claims and study them under a wider range of configurations. Our experiments on in-domain and cross-domain adaptation reveal the importance of exploration and reward scaling.
to learn misspelling patterns. We propose a method to learn word embeddings that are resilient to misspellings. Existing word embeddings have limited applicability to malformed texts, which contain a non-negligible amount of out-of-vocabulary words.
models on various NLP tasks, it is still unclear what these models really learn. In this paper, we contribute to the current efforts of explaining such models by exploring the continuum between function and content words with respect to contextualization in BERT.
ization of medical studies, and release MS2 (Multi-Document Summarization of Medical Studies). This dataset facilitates the development of systems that can assess and aggregate contradictory evidence across multiple studies.
, which is a kind of insidious security threat against machine learning models. Backdoor attacks are a kind of insidious security threat against machine learning models. backdoor attacks are a kind of insidious security threat against machine learning models.
translation directions. Multilingual Neural Machine Translation (MNMT) enables one system to translate sentences from multiple source languages to multiple target languages. This is a paper discussing how to build MNMT systems that serve arbitrary X-Y translation directions. The model suffers from poor performance in one-to-many and many-to-many with zero-shot setup.
s a source monolingual corpus and an initial model to generate gender-specific pseudo-parallel corpora which are then filtered and added to the training data. We propose gender-filtered self-training (GFST) to improve gender translation accuracy on unambiguously gendered inputs. We propose gender-filtered self-training (GFST) to improve gender translation accuracy on unambiguously gendered inputs.
, and we introduce a novel two-stage label decoding framework to model long-term label dependencies. In this work, we introduce a novel two-stage label decoding framework to model long-term label dependencies. a base model first predicts draft labels, and then a novel two-stream self-attention model makes refinements on these draft predictions based on long-range label dependencies.
,,,, and, and events. We propose Event-Centric Indicator Measure (ECIM), a novel approach to measure socio-economic indicators with events. In this paper, we propose Event-Centric Indicator Measure (ECIM), a novel approach to measure socio-economic indicators with events.
, which aims to classify the attitude of an opinionated text towards a given target. a semantic-emotion graph representation is constructed from external semantic and emotion lexicons. a learned semantic-emotion graph representation, which serves as prior knowledge bridging the gap between the source and target domains, is fully integrated into the BiLSTM cell.
for Open-Domain Passage Retrieval (ODPR). Existing studies focus on further optimizing by improving negative sampling strategy or extra pretraining.
, demonstrating that our proposed model consistently outperforms various baselines. Detailed analysis on different matching strategies demonstrates that it is essential to learn suitable matching weights to emphasize useful features.
,, a simple and effective method that (after an initial wait) commits one output word on receiving each input word, making beam search seemingly inapplicable. To address this challenge, we propose a new speculative beam search algorithm that hallucinates several steps into the future. This idea makes beam search applicable for the first time to the generation of a single word in each step.
, summaries, and summaries are converted into proxy queries. We propose to decompose QFS into query focused summarization (QFS).
, and dictionaries, the models largely outperform baselines that rely more naively rely on bilingual embeddings or dictionaries for cross-lingual transfer. We introduce a cross-lingual relation classifier trained only with English examples and a bilingual dictionary. Our classifier relies on a novel attention-based distillation approach to account for translation ambiguity when transferring knowledge from English to cross-lingual settings.
capture technology on the ground in an Australian Aboriginal community. We reflect on our interactions with participants and draw lessons that apply to anyone seeking to develop methods for language data collection in an Indigenous community.
dialogues, we show that adding sociolinguistically-rounded speaker features as prepended prompts significantly improves accuracy. We hypothesize that enriching models with speaker information in a controlled, educated way can guide them to pick up on relevant inductive biases.
-based models using only a few target samples can cause over-fitting. This can be quite limiting as most languages in the world are under-resourced.
pretraining, pretraining pretraining in controlled study. We compare three summarization specific pretraining objectives with the task agnostic corrupted span prediction pretraining in controlled study. We compare three summarization specific pretraining objectives with the task agnostic corrupted span prediction pretraining in controlled study.
of a sequence model using decomposed sub-tasks. These hidden intermediates can be improved using beam search to enhance the overall performance.
and local information from different modalities and dynamic inter-company relationships networks. In this paper, we propose a model that captures both global and local multimodal information for investment and risk management-related forecasting tasks.
, we propose a fine-grained knowledge fusion model with the domain relevance modeling scheme. Besides, a significant characteristic of sequence labeling tasks is that different elements within a given sample may also have diverse domain relevance.
,,, ands, and our method outperforms all the strong baselines under different latency, including the state-of-the-art adaptive policy.
summarisation corpus with long documents in a source language associated with multi-sentence summaries in a target language. We present a cross-lingual summarisation corpus with long documents in a source language associated with multi-sentence summaries in a target language. The corpus covers twelve language pairs and directions for four European languages, namely Czech, English, French and German, and the methodology for its creation can be applied to several
, we propose *iterative realignment*, which by refining latent alignments allows more flexible edits in less steps. Our model, Align-Refine, is an end-to-end Transformer which iteratively realigns connectionist temporal classification (CTC) alignments. On the WSJ dataset, Align-Refine matches an autoregressive baseline with a 14x decoding speedup
and hyperbolic models based on the power-law dynamics of cryptocurrencies and user behavior on social media. Taking the first step towards NLP for cryptocoins, we present and publicly release CryptoBubbles, a novel multi- span identification task for bubble detection. CryptoBubbles is a novel multi-span identification task for bubble detection, and a dataset of more than 400 cryptocoins from 9 exchanges over five years
and text simplification. We propose a new task of expertise style transfer and contribute a manually annotated dataset. Solving this task is a challenging task, unaddressed in previous work.
in conversational question answering, most prior work does not focus on follow-up questions. Despite recent progress in conversational question answering, most prior work does not focus on follow-up questions. in this paper, we introduce a new follow-up question identification task.
RankNAS can design high-performance architectures while being orders of magnitude faster than state-of-the-art NAS systems.
s in LMs.ss in LMs.s in LMs.s in LMs. linguistic theories, argument structure is assumed to be predictable from the meaning of verbs. The verb is the primary determinant of the meaning of a clause. In contrast, construction grammarians propose that argument structure is encoded in constructions (or form-meaning pairs) that are distinct from verbs.
,,, and demonstrate the effectiveness of our approach on the problem of predicting perpetrators in crime drama series. We introduce two tasks, crime case and speaker type tagging, that contribute to movie understanding and demonstrate the effectiveness of our model on them.
, and it remains an open question to what extent modern language models can interpret nonliteral phrases. To address this question, we introduce Fig-QA, a Winograd-style nonliteral language understanding task. We evaluate the performance of several state-of-the-art language models on this task.
,,,,, and (2) claim-evidence pair extraction (CEPE). We propose two new integrated argument mining tasks associated with the debate preparation process. We propose two new integrated argument mining tasks associated with the debate preparation process.
, and, and, and yielding competitive results on entity linking and slot filling, by generating disambiguated text. KILT data and code are available at https://github.com/facebookresearch/KILT.
to identify argumentative discourse structures and embed them in an adaptive writing support system for students. We introduced an argumentation annotation approach to model the structure of argumentative discourse in student-written business model pitches.
, analysis and word-sense disambiguation. Subjectivity is the expression of internal opinions or beliefs which cannot be objectively observed or verified. Subjectivity is the expression of internal opinions or beliefs which cannot be objectively observed or verified. Subjectivity is an important aspect of user-generated data.
to improve performance of model tuning., we explore “prompt tuning” to condition frozen language models to perform specific downstream tasks., we explore “soft prompts” to condition frozen language models to perform specific downstream tasks., we explore “prompt tuning” a simple yet effective mechanism for learning “soft prompts” to condition frozen language models to perform specific downstream tasks.
and multi-source translation tasks. We propose a two-stage finetuning method to alleviate the pretrain-finetune discrepancy. We propose a two-stage finetuning method to alleviate the pretrain-finetune discrepancy. We propose a novel MSG model with a fine encoder to learn better representations in MSG tasks.
, we use the biaffine parser to identify relation extraction tasks. This paper aims to adapt the popular dependency parsing model, the biaffine parser, to this entity relation extraction task.
s are language features that imply the negation of stronger statements, e.g., “She was married twice” typically implicates that she was not married thrice. In this paper we discuss the importance of scalar implicatures in the context of textual information extraction.
,,, and QA benchmarks. AISO is a new approach for open-domain question answering, namely AISO. In this paper, we propose a novel adaptive information-seeking strategy for open-domain question answering. AISO could adaptively select a proper retrieval action to seek the missing evidence at each step.
, and,,, resulting in a biased sample of the minority class. Here, we present a large-scale empirical study on active learning techniques for BERT-based classification. We present a large-scale empirical study on active learning techniques for BERT-based classification.
, and a sentiment reversal comes also a reversal in meaning. Positive reframing is a challenging and semantically rich task. Positive psychology frames is a benchmark for positive reframing.
s to obtain high-quality sentence embeddings from pretrained language models (PLMs). a large set of labeled text pairs must be augmented with additional pretraining objectives or finetuned on a large set of labeled text pairs. a large set of labeled text pairs must be finetuned on a large set of labeled text pairs.
based geometric approach for learning unsupervised alignment of word embeddings between the source and the target languages. We propose a novel manifold based geometric approach for learning unsupervised alignment learning problem.
s (BM25) achieves comparable or better performance than state-of-the-art dense retrieval methods such as DPR. GAR achieves state-of-the-art performance on Natural Questions and TriviaQA datasets.
,, a neural baseline that outperforms a graph-based state-of-the-art method for binary sub-event detection (2.7% micro-F1 improvement) and (ii) demonstrate superiority of a recurrent neural network model on the posts sequence level for labeled sub-events (2.4% bin-level F1 improvement)
,, and,, and, and performance, surpassing prior work, and in some cases even outperforming supervised approaches. Code is made available at https://github.com/pytorch/fairseq/examples/MMPT.
and a MAP inference to find target segments with maximum posterior probability. DPE uses a lightweight mixed character-subword transformer as a means of pre-processing parallel data to segment output sentences using dynamic programming.
, we find simple manipulation of attention temperatures in Transformers can make pseudo labels easier to learn for student models.
, and the variability of this confidence across epochs. We introduce Data Maps—a model-based tool to characterize and diagnose datasets. We leverage a largely ignored source of information: the behavior of the model on individual instances during training (training dynamics) for building data maps.
,, and a cohesive text by reconstructing these salient facts into a shorter summary that faithfully reflects these complex relations. In this paper, we adapt TP-Transformer (Schlag et al., 2019), an architecture that enriches the original Transformer (Vaswani et al., 2017) with the explicitly compositional Tensor Product Representation (TPR) for the task of abstractive
-based language model fine-tuning and present a state-of-the-art prompt-based few-shot learner, SFLM. SFLM generates a pseudo label on weakly augmented version. the model predicts the same pseudo label when fine-tuned with strongly augmented version.
helpful sentences from reviews for a given product. We propose a novel task of extracting a single representative helpful sentence from a set of reviews for a given product. The selected sentence should meet two conditions: first, it should be helpful for a purchase decision and second, the opinion it expresses should be supported by multiple reviewers.
s,s, and human-like dialogues.s,s, and human-like dialogues.-oriented dialogues, explicitly trained to predict user goals, and to generate contextually relevant chit-chat responses. Lastly, we propose three new models for adding chit-chat to task-oriented dialogues, explicitly trained to predict user goals.
augmentation paradigm termed Continuous Semantic Augmentation (CsaNMT) which augments each training instance with an adjacency semantic region. augmentations augment each training instance with an adjacency semantic region that could cover adequate variants of literal expression under the same meaning.
, and,,, and transliterated inputs. This paper demonstrates that incorporating alternative inputs via self-ensemble can be particularly effective when training set is small, leading to +5 BLEU when only 5% of the total training data is accessible. Finally, our analysis shows that incorporating alternative inputs via self-ensemble can be particularly effective when training set is small, leading to +5 BLEU when only 5% of the total
, we present a novel and extensive approach, which formulates it as a semantic segmentation task. Instead of generating from scratch, such a formulation introduces edit operations and shapes the problem as prediction of a word-level edit matrix.
and text embeddings for joint reasoning. We use COVID-19 as a case study.
,, and,,s and on possible methods to approach such issues. on interpolating neural networks. Neural networks are surprisingly good at interpolating and perform remarkably well when the training set examples resemble those in the test set. However, they are often unable to extrapolate patterns beyond the seen data, even when the abstractions required for such patterns are simple.
, and we show how learning a word-to-word or word-to-sentence relatedness score can improve the performance of text spotting systems. We present a scenario where semantic similarity is not enough, and devise a neural approach to learn semantic relatedness. We show how learning a word-to-word or word-to-sentence relatedness score can improve the performance of text spotting systems up to 2.9 points
, a reference-free metric that trains unsupervised models to measure several desirable qualities of dialog. USR is a reference-free metric that trains unsupervised models to measure several desirable qualities of dialog.
, we propose to regularize the parser with phrases extracted by an unsupervised phrase tagger. This work is inspired by human reading process of human. We propose to regularize the parser with phrases extracted by an unsupervised phrase tagger.
, a model that integrates relational and distributional signals, forming an effective sub-space representation for each relation.
in grammatical and stylistic consistency. Stories generated with neural language models have shown promise in grammatical and stylistic consistency. Stories generated with neural language models have shown promise in grammatical and stylistic consistency.
for targeted words. We propose a controllable target-word-aware model for this task. Our proposed model can generate reasonable examples for targeted words, even for polysemous words.
and its surrounding discourse. We train simple gradient boosting classifiers on representations of an utterance and its surrounding discourse learned with a variety of document embedding methods. obtaining near state-of-the-art results on the 2018 VU Amsterdam metaphor identification task without the complex metaphor-specific features or deep neural architectures employed by other systems.
,, and personality traits based on their previous activity, without dependence on explicit user profiles or questionnaires. Using a dataset of over 60,000 argumentative discussions, we demonstrate that our modeling of debater’s characteristics enhances the prediction of argument persuasiveness.
and ATP, a new attacking paradigm to measure robustness of Text-to-SQL models. We propose the Adversarial Table Perturbation (ATP) as new attacking paradigm to measure robustness of Text-to-SQL models. We propose the Adversarial Table Perturbation (ATP) as new attacking paradigm to measure robustness of Text-to-SQL models.
. This is a novel memory architecture that combines memory architectures to incorporate external knowledge in dialogs. This paper aims to relax the strong assumptions made by existing architectures.
, we use hierarchical structure to model post-wise information incorporating contextual knowledge.
using BERT-based models across five datasets. We evaluate models on their generalizability to out-of-domain examples, responses to missing or incorrect data, and ability to handle question variations.
ands ands and ands and and demonstrate that the neural model performance can be significantly improved by making it aware of its rationalized predictions, particularly in low-resource settings.
classification with only document-level weak supervision. To this end, we propose a novel Diversified Multiple Instance Learning Network (D-MILN) which is able to achieve aspect-level sentiment classification with only document-level weak supervision.
, we find that the latent vector was able to capture such global features in an unsupervised manner. We question this claim. We measure which words benefit most from the latent information by decomposing the reconstruction loss per position in the sentence.
summarization. We propose reinforcement learning rewards to improve factual consistency and answer coverage.
discovery aims to discover new relations from a given text corpus without annotated data. However, it does not consider existing human annotated knowledge bases even when they are relevant to the relations to be discovered.
,,,,,, parallel data is a major obstacle for training high-quality machine translation systems for low-resource languages. Unfortunately, some low-resource languages are linguistically related or similar to high-resource languages; these related languages may share many lexical or syntactic structures. In this work, we exploit this linguistic overlap to facilitate translating to and from a low-resource language with only
decoders in both blackbox and whitebox ways by studying on the entropy of the model’s token-level predictions. In this work, we analyze summarization decoders in both blackbox and whitebox ways by studying on the entropy of the model’s token-level predictions.
a new multi-modal Transformer model, dubbed as GTR, proposes a novel end-to-end multi-modal Transformer model. GTR has two encoders for video and language encoding, and a cross-modal decoder for grounding prediction.
, i.e., a hypothesis generator and a reasoner. Each hypothesis is verified by the reasoner, and the valid one is selected to conduct the final prediction.
coding. a Co-occurrence method to leverage the code hierarchy. Specifically, we propose a hyperbolic representation method to leverage the code hierarchy. Moreover, we propose a graph convolutional network to utilize the code co-occurrence.
, but current image-text grounding approaches require detailed annotations. Such granular annotation is rare, expensive, and unavailable in most domain-specific contexts. granular annotation is rare, expensive, and unavailable in most domain-specific contexts.
models are as predictive of human eye fixation patterns during task-reading as classical cognitive models of human attention. We compare attention functions across two task-specific reading datasets for sentiment analysis and relation extraction. We find whether self-attention in large-scale pre-trained language models is as predictive of human eye fixation patterns during task-reading as classical cognitive models of human attention.
a given answer to a question. We propose a system that finds the strongest supporting evidence for a given answer to a question. We train evidence agents to select the passage sentences that most convince a pretrained QA model of a given answer. Rather than finding evidence that convinces one model alone, we find that agents select evidence that generalizes. QA models can generalize to longer passages and harder questions.
, the clear top performer when there are only a few dozen training instances or less. In experiments with two English text classification datasets, we demonstrate substantial performance gains from combining pre-training with rationales.
, compression. compression. compression. compression. compression. to effectively compress BERT by progressive module replacing. to compress BERT by progressive module replacing. module replacement. Our approach divides the original BERT into several modules and builds their compact substitutes. We randomly replace the original modules with their substitutes to train the compact modules to mimic the behavior of the original modules.
, we propose a novel Dynamic Fusion Network (DF-Net) which automatically exploit the relevance between the target domain and each domain. Results show that our models outperform existing methods on multi-domain dialogue.
sssssss—e.g. syntactic structure—in contextual representations. Probes are models devised to investigate the encoding of syntactic knowledge—e.g. syntactic structure—in contextual representations. Probes are models devised to investigate the encoding of syntactic knowledge—e.g. syntactic structure—in contextual representation
to convert a transformer-based model into one that can build an information-capturing, task-independent, and compressed representation of text. The resulting representation is reusable across different examples and tasks, thereby requiring a document shared across many examples or tasks to only be read once. This leads to faster training and evaluation of models.
models for word segmentation, especially Chinese, because of the ability to minimize the effort in feature engineering. Our model achieves better performance than the state-of-the-art models on both Japanese and Chinese benchmark datasets.
and. We examine the effectiveness of a simple, word-level Siamese architecture augmented with attention-based mechanisms for capturing semantic “soft” matches between query and post tokens. Extensive experiments on datasets from the TREC Microblog Tracks show that our simple models achieve better effectiveness than existing approaches that are far more complex or exploit a more diverse set of relevance signals.
that the bias is _amplified_ in the language models, like training data with gender imbalances, we find that bias is _amplified_ in the language models. We introduce new technique for measuring bias in models, using Bayesian approximations to generate partially-synthetic data from the model itself.
ssss to target domains with only unlabeled data. We propose MaskAugment, a controllable mechanism that augments text input by using the pre-trained Mask token from BERT model. In this work, we investigate how to better adapt DA taggers to desired target domains with only unlabeled data.
8%. The superiority on parsing results verifies the feasibility of follow-up query analysis. To accomplish the task, we propose STAR, a novel approach with a well-designed two-phase process. STAR is parser-independent and able to handle multifarious follow-up scenarios in different domains.
rewriting aims to reconstruct the latest dialogue utterance by copying the missing content from the dialogue context. Until now, the existing models for this task suffer from the robustness issue. i.e., performances drop dramatically when testing on a different dataset.
achieve the latest performance on many translation benchmarks. Our approaches yield up to +1.28 and +0.89 BLEU points improvements over the Transformer baseline, respectively.
pretrain-finetune pipeline with an extra embedding transfer step. We extend the vanilla pretrain-finetune pipeline with an extra embedding transfer step. Specifically, a plug-and-play embedding generator is introduced to produce the representation of any input token.
ssss. We propose the use of informative priors to create interpretable and domain-informed dimensions for probabilistic word embeddings. We propose the use of informative priors to create interpretable and domain-informed dimensions for probabilistic word embeddings.
in,,, in and test their assumptions. We release tools for researchers to conduct their own power analysis and test their assumptions. We make recommendations for improving statistical power.
that ignores values, in the context of language modeling. We propose a value-aware objective, and show that the choice of kernel function for computing attention similarity can substantially affect the quality of sparse approximations.
,,,, and on two benchmark datasets WN18RR and FB15k-237, and outperforms strong search personalization baselines on SEARCH17.
by generating extra labeled sequences in each iteration. The key difficulty is to generate plausible sequences along with token-level labels. SeqMix can improve the standard active sequence labeling method by 2.27%–3.75% in terms of F1 scores.
based on pretrained language models have achieved remarkable performance on various benchmark datasets. QA models based on pretrained language models have achieved remarkable performance on various benchmark datasets. DA techniques which drop/replace words have shown to be effective in regularizing the model from overfitting to the training data.
models often exploit biases to achieve high dataset-specific performance without properly learning the intended task. Recently proposed debiasing methods are shown to be effective in mitigating this tendency. However, these methods rely on a major assumption that the types of bias should be known a-priori, which limits their application to many NLU tasks and datasets.
,, the that the model often prefers an empty translation. We show by constraining search with a minimum translation length that at the root of the problem of empty translations lies an inherent bias towards shorter translations.
and understanding, we propose a new method for neural sequence modeling that takes partially-observed sequences of discrete, external knowledge into account. We construct a sequential neural variational autoencoder, which uses Gumbel-Softmax reparametrization within a carefully defined encoder, to allow for successful backpropagation during training.
from different sources has led to better results than when using such data in isolation. In this work we analyse the impact that backtranslated data from different sources has on new MT systems.
,, the state-of-the-art model used in industry on both patents and papers..-based model, and effective baseline for classifying both patents and papers to the well-established Cooperative Patent Classification (CPC)., we provide a simple and effective baseline for classifying both patents and papers to the well-established Cooperative Patent Classification (CPC)., structure, where the Wide part
and and unsupervised settings. We evaluate the proposed method in both supervised and unsupervised settings. Our analysis reveal the superiority of the proposed model in both settings.
, which are implemented with a flexible and universal Gumbel-Softmax. In this paper, we bridge the gap by assessing the strengths of selective SANs (SSANs) with selective mechanism. Specifically, the selective mechanism improves SANs by paying more attention to content words that contribute to the meaning of the sentence.
ization is a task that facilitates user-guided exploration of information within a document set. To that end, we propose two novel deep reinforcement learning models for the task that address all of the task requirements.
,, and, we developed a knowledge-guided reinforcement learning framework for open attribute value extraction. In this work, we propose a knowledge-guided reinforcement learning framework for open attribute value extraction. In this work, we propose a knowledge-guided reinforcement learning framework for open attribute value extraction.
spelling errors belong to phonological or visual errors. To address these issues, we propose a novel end-to-end trainable model called PHMOSpell, which promotes the performance of CSC with multi-modal information. Specifically, we derive pinyin and glyph representations for Chinese characters from audio and visual modalities respectively.
extraction for the biomedical domain is more challenging than that in the general news domain. To better encode contextual information and external background knowledge, we propose a novel knowledge base (KB)-driven tree-structured long-term memory networks (Tree-LSTM) framework.
training datasets.We develop two tools that allow us to deduplicate training datasets—for example removing from C4 a single 61 word English sentence that is repeated over 60,000 times.Deduplication allows us to train models that emit memorized text ten times less frequently and require less training steps to achieve the same or better accuracy.Code for deduplication is released at https://github.com/google-research/de
to the context of dialog similarity. We propose a novel adaptation of the edit distance metric to the scenario of dialog similarity. We evaluate this new approach and compare it to existing document similarity measures on two publicly available datasets.
s are very challenging due to (i) the complexity of videos which contain both spatial and temporal variations, and (ii) the complexity of user utterances which query different segments and/or objects in videos over multiple dialogue turns. BiST achieves competitive performance and generates reasonable responses on a large-scale AVSD benchmark.
and concepts. The edges are from entities to concepts, showing that an entity is an instance of a concept. Concept graphs are created as universal taxonomies for text understanding in the open-domain knowledge. The nodes in concept graphs include both entities and concepts. The edges are from entities to concepts, showing that an entity is an instance of a concept.
on popular multi-hop datasets. Does Quark's performance on popular multi-hop datasets really justify this added design complexity? Our results suggest that the answer may be no, because even our simple pipeline based on BERT, named, performs surprisingly well.
, with gains of around 500% over the strongest state-of-the-art baselines. In our evaluation, considering twelve datasets and seven state-of-the-art baselines, CluHTM outperformed the baselines in the vast majority of the cases.
, we propose an unsupervised method to detect the stance of argumentative claims with respect to a topic. In this paper, we propose an unsupervised method to detect the stance of argumentative claims with respect to a topic. In this paper, we propose an unsupervised method to detect the stance of argumentative claims with respect to a topic.
, LABES-S2S, and LABES-S2S. In supervised experiments, LABES-S2S significantly outperforms both supervised-only and semi-supervised baselines. LABES-S2S is a copy-augmented Seq2Seq model instantiation of LABES.
ACE2005 show that our model outperforms nine strong baselines, is especially effective for unseen/sparsely labeled trigger words. To address the issue, we propose a novel Enrichment Knowledge Distillation (EKD) model to leverage external open-domain trigger knowledge to reduce the in-built biases to frequent trigger words in annotations.
ss have been widely applied and recently proved vulnerable under backdoor attacks: the released pre-trained models can be maliciously poisoned with certain triggers. When triggers are activated, even the fine-tuned model will predict pre-defined labels, causing a security threat.
,, and. We propose a simple and elegant framework named SongNet to tackle this problem. We propose a simple and elegant framework named SongNet to tackle this problem. The backbone of the framework is a Transformer-based auto-regressive language model. Sets of symbols are tailor-designed to improve the modeling performance especially on format, rhyme, and sentence integrity.
, and compare a variety of existing methods in both cases of non-adversarial and adversarial users that expose their weaknesses. We introduce a new human-and-model-in-the-loop framework for evaluating the toxicity of such models. We then go on to propose two novel methods for safe conversational agents.
s,,,,, and,s, a family of string transduction models defining joint and conditional probability distributions over pairs of strings. We introduce neural finite state transducers (NFSTs), a family of string transduction models defining joint and conditional probability distributions. The probability of a string pair is obtained by marginalizing over all its accepting paths in a finite state transduc
sssss. We propose a simple, effective transition-based model with generic neural encoding for discontinuous NER. We propose a simple, effective transition-based model with generic neural encoding for discontinuous NER.
definition generation. Graphine covering 2,010,648 terminology definition pairs, spanning 227 biomedical subdisciplines. Terminologies in each subdiscipline further form a directed acyclic graph, opening up new avenues for developing graph-aware definition generation models.
sssss: Text summarization is one of the most challenging and interesting problems in NLP. Text summarization is one of the most challenging and interesting problems in NLP. We discussed specific challenges that current approaches faced with this task.
-agnostic and model-specific explanation methods for CNNs for text classification. In this paper, we consider several model-agnostic and model-specific explanation methods for CNNs for text classification. We conduct three human-rounded evaluations, focusing on different purposes of explanations: (1) revealing model behavior, (2) justifying model predictions, and (3) helping humans investigate uncertain predictions. The results highlight dissimilar qualities of the various explanation methods we consider and
,, and,, and (3) MART-based transformers to model complex interactions between frames. We present ablation studies to demonstrate the effect of each of these techniques on the generative power of the model. We present a number of improvements to prior modeling approaches, including (1) the addition of a dual learning framework that utilizes video captioning to reinforce semantic alignment between the story and generated images.
s, and Span-Fact, a suite of two factual correction models that leverage knowledge learned from question answering models to make corrections in system-generated summaries via span selection. Span-Fact, a suite of two factual correction models that leverage knowledge learned from question answering models to make corrections in system-generated summaries via span selection.
in text classification tasks. We propose WordDP to achieve certified robustness against word substitution at- tacks in text classification tasks. In this paper, we propose WordDP to achieve certified robustness against word substitution at- tacks in text classification via differential privacy (DP).
, we present a new framework of conversational machine reading that comprises a novel Explicit Memory Tracker (EMT) to track if conditions listed in the rule text have already been satisfied to make a decision. Moreover, our framework generates clarification questions by adopting a coarse-to-fine reasoning strategy.
, and counterfactual decoders. The attentional encoder leverages the plaintiff’s claim and fact description as input to learn a claim-aware encoder. the attentional encoder leverages the plaintiff’s claim and fact description as input to learn a claim-aware encoder. the attentional encoder leverages the plaintiff’s claim and fact description as input to learn a claim-aware encoder.
s and low-frequency (LF) entities. We present delexicalized entity identification to explicitly extract their frequency-agnostic and entity-type-specific representations. For OOV entities, we introduce local context reconstruction to implicitly incorporate contextual information into their representations.
of word meaning. We propose a method for mapping human property knowledge onto a distributional semantic space. Our approach gives a measure of concept and feature affinity in a single semantic space. Our approach gives a measure of concept and feature affinity in a single semantic space.
contradiction
decoding for text style transfer., we propose a base NAR model by directly adapting the common training scheme from its AutoRegressive (AR) counterpart. This model sacrifices its transfer performance due to the lack of conditional dependence between output tokens.
, a dependency parser. We construct the graph from n-grams extracted from a lexicon built from pointwise mutual information (PMI) and apply attention over the graph. Therefore, different word pairs from the contexts within and across n-grams are weighted in the model and facilitate RE accordingly. Experimental results with further analyses on two English benchmark datasets for RE demonstrate the effectiveness of our approach, where state-of-the
, entmax, and n-gram diversity. This is a text generator with favorable performance in terms of fluency and consistency. -perplexity, sparsemax score, and Jensen-Shannon divergence.
tag dependency and perform POS induction through the objective of masked POS reconstruction. To better understand this phenomenon, we propose a Masked Part-of-Speech Model (MPoSM) inspired by the success of Masked Language Models (MLM).
ss induction and cross-lingual word similarity, our approach can achieve competitive or superior performance compared to state-of-the-art published results. We argue that this formulation has several intuitively attractive properties, particularly with the respect to improving robustness and generalization to mappings between difficult language pairs or word pairs.
and performance: the Pareto hypervolume. To measure complexity, we present a number of parametric and non-parametric metrics. To measure complexity, we present a number of parametric and non-parametric metrics. These results lead us to argue, second, that common simplistic probe tasks such as POS labeling and dependency arc labeling, are inadequate to evaluate properties encoded in contextual word representations.
,, and the human judgements. We present an extensive human evaluation study of consultation notes where 5 clinicians (i) listen to 57 mock consultations, (ii) write their own notes, (iii) post-edit a number of automatically generated notes, and (iv) extract all the errors, both quantitative and qualitative. We then carry out a correlation study with 18 automatic quality metrics and the human judgements.
,,,, in some constructions. We find that these models perform even more poorly than our LSTMs in some constructions. We find that these models perform even more poorly than our LSTMs in some constructions.
,, and,; (ii) use as few-shot parsing as possible for model development; (ii) use as few-shot parsing trained on the same labeled examples. We propose two protocols for future work on unsupervised parsing: (i) use fully unsupervised criteria for hyperparameter tuning and model selection; (ii) compare to few-shot parsing trained on the same labeled examples
, we are the first to train from EMG collected during vocalized speech. We introduce a method of training on silent EMG by transferring audio targets from vocalized to silent signals.
in the neural generation model. We develop a novel NLG model to produce a target sequence based on a given list of entities.
in the world, we use embeddings derived from articulatory vectors rather than embeddings derived from phoneme identities to learn phoneme representations. this enables us to fine-tune a high-quality text-to-speech model on just 30 minutes of data in a previously unseen language spoken by a previously unseen speaker.
,ationationationationation. Lexical disambiguation is a major challenge for machine translation systems. Lexical disambiguation is a major challenge for machine translation systems, especially if some senses of a word are trained less often than others.
to generate human-like questions. We propose to incorporate paraphrase knowledge into question generation(QG) to generate human-like questions. We present a two-hand hybrid model leveraging a self-built paraphrase resource, which is automatically conducted by a simple back-translation method.
- vs. domain-specific word embeddings. We propose two novel models to exploit general- vs. domain-specific comparisons. a simple neural network model with pre-computed comparative-embedding information as input, and a multi-channel model computing the comparison internally.
aims to empower machines with the human ability to make presumptions about ordinary situations. Commonsense reasoning aims to empower machines with the human ability to make presumptions about ordinary situations. Commonsense reasoning aims to empower machines with the human ability to make presumptions about ordinary situations.
, ProgModel. ProgModel is a novel progressive slot filling model, ProgModel. ProgModel consists of a novel context gate that transfers previously learned knowledge to a small size expanded component. ProgModel learns the new knowledge by only using new data at each time and meanwhile preserves previously learned expressions.
-based sentiment analysis (ABSA) is a challenging subtask of sentiment analysis (SA). In this paper, we construct an auxiliary sentence from the aspect and convert ABSA to a sentence-pair classification task.
, and three SRL benchmark datasets, including both dependency- and span-based SRL, we demonstrate the effectiveness of syntactic supervision in low-resource scenarios.
, and text classification datasets without sacrificing predictive performance. Finally, we propose a new family of Task-Scaling (TaSc) mechanisms that learn task-specific non-contextualised information to scale the original attention weights. Attention has empirically been demonstrated to improve performance in various tasks, while its weights have been extensively used as explanations for model predictions.
determining indirect answers to yes-no questions in real conversations.
ation mining is to automatically extract argumentation structures from argumentative texts. The goal of argumentation mining is to extract argumentation structures from argumentative texts.
, German translation dataset, MAE improves over “transformer-base” by 0.8 BLEU, with a comparable number of parameters. In this work, we instead “reallocate” them—the model learns to activate different heads on different inputs.
and experimental results. We present new annotations on top of corpora annotating possession existence and experimental results. Regarding possession existence, we derive the time spans we work with empirically from annotations indicating lower and upper bounds.
and diagram multiple choice questions. ISAAQ reports unprecedented success in all TQA question types, with accuracies of 81.36%, 71.11% and 55.12% on true/false, text-only and diagram multiple choice questions. ISAAQ also demonstrates its broad applicability, obtaining state-of-the-art results in other demanding datasets.
to improve relation extraction with joint label embedding. The model makes full use of both structural information from Knowledge Graphs and textual information from entity descriptions. The learned label embeddings are used as another atten-tion over the instances (whose embeddings are also enhanced with the entity descriptions) for improving relation extraction.
multi-hop question generation focuses on generating complex questions that require reasoning over multiple pieces of information of the input passage. Current models with state-of-the-art performance have been able to generate the correct questions corresponding to the answers. To address this challenge, we propose the CQG, which is simple and effective controlled framework.
, we explore how to incorporate structured domain knowledge for the Medical NLI task. In this paper, we explore how to incorporate structured domain knowledge, available in form of a knowledge graph (UMLS), for the Medical NLI task. In this paper, we explore how to incorporate structured domain knowledge, available in the form of a knowledge graph (UMLS), for the Medical NLI task.
to achieve the domain adaptability. We propose a hierarchical model to learn the global context for document-level neural machine translation (MT). this is done through a sentence encoder to capture intra-sentence dependencies and a document encoder to model document-level inter-sentence consistency and coherence.
, we propose a more efficient approach to handle table-related tokens before the semantic parser. Firstly, we formulate it as a sequential tagging problem and propose a two-stage anonymization model to learn the semantic relationship between tables and input utterances. Secondly, we propose a two-stage anonymization model to learn the semantic relationship between tables and input utterances.
, and we propose a variant of WAE that encourages the stochastically encoded Gaussian distribution. We propose to use the Wasserstein autoencoder (WAE) for probabilistic sentence generation. We show theoretically and empirically that, in the original WAE, the stochastically encoded Gaussian distribution tends to become a Dirac-delta function.
DS can be re-emphasized in documents and significantly harm the performance of RE. To alleviate this issue, we propose a novel pre-trained model for DocRE, which de-emphasize noisy DS data via multiple pre-training tasks.
labels, and how they directly contribute to label prediction remains unclear. We find inspiration from biologists and study the affinity between individual neurons and labels. We find inspiration from biologists and study the affinity between individual neurons and labels.
. We present a novel framework to automatically recommend conversations to users based on their prior conversation behaviors.
contexts. We propose a more concise encoding for background context structured in the form of knowledge graphs. This is done by creating pseudo utterances through paraphrasing knowledge triples, added into the accumulated dialogue context.
,,,,, linguistic patterns and context-dependent reasoning. We also show that AAs learn linguistic patterns and context-dependent reasoning.
, and (ii) perform joint reasoning over the QA context and KG. Here we propose a new model, QA-GNN, which addresses the above challenges through two key innovations: (i) relevance scoring, where we use LMs to estimate the importance of KG nodes relative to the given QA context, and (ii) joint reasoning over the QA context and KG.
ssssss and slots. We investigate how to approach DST using a generation framework without the pre-defined ontology list. This paper explores how to approach DST using a generation framework without the pre-defined ontology list. This paper explores how to approach DST using a generation framework without the pre-defined ontology list.
to natural language descriptions. Text generation from a knowledge base aims to translate knowledge triples to natural language descriptions. Most existing methods ignore the faithfulness between a generated text description and the original table, leading to generated information that goes beyond the content of the table.
0Shot-TC. The 0Shot-TC problem is a challenging NLU problem. 0Shot-TC aims to associate an appropriate label with a piece of text, irrespective of the text domain and the aspect (e.g., topic, emotion, event, etc.) described by the label. 0Shot-TC aims to combine an appropriate label with a piece of text, irrespective of the
,,, and the response appropriateness score of 4.287. This paper presents an end-to-end neural architecture for dialogue systems that addresses both challenges above. In the human evaluation, our dialogue system achieved the success rate of 68.32%, the language understanding score of 4.149, and the response appropriateness score of 4.287. This ranked the system at the top position in the end-to-end multi-domain dialogue system task in the
, we introduce the new algorithm S-DIORA, an improved variant of DIORA that encodes a single tree rather than a softly-weighted mixture of trees. To fix this issue, we introduce S-DIORA, an improved variant of DIORA that encodes a single tree rather than a softly-weighted mixture of trees.
based on entities improves constituency parsing on English WSJ Penn Treebank by more than 5 F1. Using relatively small number of span constraints we can substantially improve the output from DIORA, an already competitive unsupervised parsing system.
s for a word in a given textual context. Lexical substitution is the task of generating meaningful substitutes for a word in a given textual context. lexical substitution is the task of generating meaningful substitutes for a word in a given textual context.
,,,,,, and convolutional neural networks can efficiently extract local and global context directly from the target domain.: (i) the deep Transformer-based pre-trained models, utilized via the mixed-domain transfer learning, are only good at capturing the local and global context of the target domain. ii) a combination of shallow network-based pre-trained models and convolutional neural networks can efficiently
in research publications. We used this prediction model to retrieve specific health advice on COVID-19 treatments. We used this prediction model to retrieve specific health advice on COVID-19 treatments.
, and a method that can help to overcome resource scarcity in cross-lingual NLP problems, by enabling fast adaptation to new tasks. Meta-learning, or learning to learn, is a technique that can help to overcome resource scarcity in cross-lingual NLP problems, by enabling fast adaptation to new tasks.
attribution methods are one way of accomplishing these goals by retrieving training instances that (may have) led to a particular prediction. Influence functions (IF; Koh and Liang 2017) provide machinery for doing this by retrieving training instances that (may have) led to a particular prediction.
blanks at specified locations. The blanks control which part of the sequence to expand, making BLM ideal for a variety of text editing and rewriting tasks. The model can start from a single blank or partially completed text with blanks at specified locations. The blanks control which part of the sequence to expand, making BLM ideal for a variety of text editing and rewriting tasks.
,, and word entropy predict code-switching in bilingual written conversation. We describe and model a new dataset of Chinese-English text with 1476 clean code-switched sentences, translated back into Chinese.
and EM models to decouple feature representation from matching decision. Using self-supervised learning and mask mechanism, KAT Induction learns an efficient decision tree that can be interpreted by generating entity matching rules.
s in printed Early Modern documents. We propose a deep and interpretable probabilistic generative model to analyze glyph shapes in printed Early Modern documents. We focus on clustering extracted glyph images into underlying templates in presence of multiple confounding sources of variance. We propose a deep and interpretable probabilistic generative model to analyze glyph shapes in printed Early Modern documents.
, we introduce knowledge base as the input context. In real-world practical, the entity may not be included by the knowledge base or suffer from the precision of knowledge retrieval.
, and Koehn (2016) with phrase-based MT, artificially reducing translation quality. MT performance and post-editing time is however not straightforward and, contrary to the results on phrase-based MT, BLEU is definitely not a stable predictor of the time or final output quality.
aims to generate a sequence of descriptive captions for each event in a long untrimmed video. In detail, the model leverages local and global context with different mechanisms to jointly learn to generate coherent captions.
,,, using BERT-based models. MATE-KD is a novel text-based adversarial training algorithm which improves the performance of knowledge distillation. MATE-KD is a novel text-based adversarial training algorithm which improves the performance of knowledge distillation. MATE-KD is a novel text-based adversarial training algorithm which improves the performance of knowledge distillation.
,, and pattern information. For event information, we generate pattern vectors for matching with sentences. For event information, we select key sentences to represent an article and then predict if the article fact-checks the given claim using the claim, key sentences, and patterns.
s from three different domains and time windows and define the task of evaluating thematic coherence. Here we create a corpus of microblog clusters from three different domains and time windows and define the task of evaluating thematic coherence.
,,-based loss-function DMI (Discourse Mutual Information) for training dialog-representation models. We argue that the structure-unaware word-by-word generation is not suitable for effective conversation modeling. We propose a structure-aware Mutual Information based loss-function DMI (Discourse Mutual Information) for training dialog-representation models.
s.-baseds... (i.e., languages with non-isomorphic monolingual spaces). We present InstaMap, an instance-based method for learning cross-lingual word embeddings. Unlike prior work, it deviates from learning a single global linear projection.
, and to inform users of the quality of automatically produced summaries and other types of generated text.
s andss with one-hot representations of each byte does not hurt performance; experiments on byte-to-byte machine translation from English to 10 different languages show consistent improvement in BLEU.
to eliminate redundant examples chosen by an AL strategy. We demonstrate that the proposed approach is further able to reduce the data requirements of state-of-the-art AL strategies by  3-25% on an absolute scale on multiple NLP tasks.
, and semantic properties. We introduce the Recursive Noun Phrase Challenge (RNPC), a dataset of three textual inference tasks involving textual entailment and event plausibility comparison. Models trained on RNPC achieve strong zero-shot performance on an extrinsic Harm Detection evaluation task.
using well-known SRL datasets. We then train our model in a cross-lingual setting to generate new SRL labeled data. Finally, we measure the effectiveness of our method by using the generated data to augment the training basis for resource-poor languages.
,,,,, multilingual mask utterance retrieval and multilingual inconsistency identification). In this work, we introduce new pretraining losses tailored to learn generic multilingual spoken dialogue representations. Our experiments show that our new losses achieve a better performance in both monolingual and multilingual settings.
CLEVE, a contrastive pre-training framework for EE to better learn event knowledge from large unsupervised data. CLEVE contains a text encoder to learn event semantics and a graph encoder to learn event structures respectively. CLEVE also includes a graph encoder to learn event semantic representations by graph contrastive pre-training on parsed event-related semantic structures.
, which identifies and leverages semantically-aligned token pairs. In this work, we explicitly describe the sentence distance as the weighted sum of contextualized token distances on the basis of a transportation problem. In this work, we propose CLRCMD, a contrastive learning framework that optimizes RCMD of sentence pairs, which enhances the quality of sentence similarity and their interpretation.
, ands to improve model explanability. the new accuracy while improving model explanability. the FB-NYT dataset to evaluate the quality of explanations afforded by relation extraction models. We demonstrate that replacing the entity mentions in the sentences with their fine-grained entity types enhances extraction accuracy.
s, such as Spanish and French. In this paper, we propose new metrics for evaluating gender bias in word embeddings of these languages and further demonstrate evidence of gender bias in bilingual embeddings which align these languages with English. This paper proposes new metrics for evaluating gender bias in word embeddings of these languages and further demonstrate evidence of gender bias in bilingual embeddings which align these languages with English.
, polarity and aspect-sentiment pairs, are predicted to be unified. SKEP conducts sentiment masking and constructs three sentiment knowledge prediction objectives. SKEP conducts sentiment masking and constructs three sentiment knowledge prediction objectives. SKEP is used in traditional sentiment analysis approaches to learn a unified sentiment representation.
. We propose an automatic method to mitigate the biases in pretrained language models. In this paper, we propose an automatic method to mitigate the biases in pretrained language models. In this paper, we propose an automatic method to mitigate the biases in pretrained language models.
for addressing complex mathematical problems is a semantically challenging task, which is still unexplored in the field of natural language processing for mathematical text. The task consists in using conjectures written in both natural language and mathematical formulae to recommend premises that most likely will be useful to prove a particular statement.
steganography studies how to hide secret messages in natural language cover texts. Traditional methods aim to transform a secret message into an innocent text via lexical substitution or syntactical modification. Recent advances in neural language models (LMs) enable us to directly generate cover text conditioned on the secret message.
, we quantify, analyze and mitigate gender bias exhibited in ELMo’s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities.
to extract parallel sentences from two monolingual corpora using neural machine translation. Our method relies on translating sentences in one corpus, but constraining the decoding by a prefix tree built on the other corpus.
mining with a neural model. We jointly predict proposition types and edges between propositions. Our proposed model incorporates (i) task-specific parameterization (TSP) that effectively encodes a sequence of propositions.
, we explore a new application, Smart-To-Do, that helps users with task management over emails. In this work, we explore a new application, Smart-To-Do, that helps users with task management over emails. We introduce a new task and dataset for automatically generating To-Do items from emails where the sender has promised to perform an action.
-language speech recognition systems. We propose a new learning method, meta-transfer learning, to transfer learn on a code-switched speech recognition system in a low-resource setting. Our model learns to recognize individual languages, and transfer them so as to better recognize mixed-language speech.
s,,,,,,,s become more pervasive, but their accountability gains value as a focal point of effort.s, such as deep learning models, can be a major drawback for their adoptions., we propose a methodology to evaluate transparency and coherence of analogy-based explanations modeling an audit stage for the system.
slang interpretation in English. We propose a semantically informed slang interpretation (SSI) framework that considers jointly the contextual and semantic appropriateness of a candidate interpretation for a query slang.
extraction is a fundamental task in Natural Language Processing, which usually contains two main parts: candidate keyphrase extraction and keyphrase importance estimation. KIEMP estimates the importance of phrase with three modules: a chunking module to measure its syntactic accuracy, information saliency, and concept consistency. whereas most existing keyphrase extraction approaches only focus on the part of them, which leads to biased results.
have pushed performance on NLP tasks to new limits, suggesting sophisticated treatment of complex linguistic inputs, such as phrases. Deep transformer models have pushed performance on NLP tasks to new limits, suggesting sophisticated treatment of complex linguistic inputs, such as phrases.
, and focuses on the self-talk process. We propose an unsupervised framework based on self-talk as novel alternative to multiple-choice commonsense tasks.
(CWS) systems has gradually reached a plateau with the rapid development of deep neural networks. This paper takes stock of what we have achieved and rethink what’s left in the CWS task. Methodologically, we propose a fine-grained evaluation for existing CWS systems. this paper could search for some promising direction for future research.
,-free domain adaptation. We compare these strategies across multiple tasks and domains. We find that active learning yields consistent gains across all SemEval 2021 Task 10 tasks and domains.
resolution, and we use a language-model-based approach for pronoun resolution. We use a language-model-based approach for pronoun resolution in combination with our WikiCREM dataset.
and,, hallucinate but even amplify hallucinations. We conduct a comprehensive human study on existing knowledge-grounded conversational benchmarks and several state-of-the-art models. We conduct a comprehensive human study on existing knowledge-grounded conversational benchmarks and several state-of-the-art models.
, and discuss several NLP tools that people from the Cherokee community have shown interest in. We propose two approaches to enrich the Cherokee language’s resources with machine-in-the-loop processing. We discuss several NLP tools that people from the Cherokee community have shown interest in.
,,, and compared the performance of multilingual OSCAR corpus, extracted from Common Crawl via language classification, filtering and cleaning. We use the multilingual OSCAR corpus, extracted from Common Crawl via language classification, filtering and cleaning, to train monolingual contextualized word embeddings (ELMo) for five mid-resource languages.
reprogramming, which extends earlier work on automatic prompt generation. We present an alternative approach based on adversarial reprogramming, which extends earlier work on automatic prompt generation. Using up to 25K trainable parameters per task, this approach outperforms all existing methods.
in detecting the date-time entities relevant to scheduling meetings. We show the efficacy of our method on the task of date-time understanding in the context of scheduling meetings.
generation. We propose a novel retrieval-based method for paraphrase generation. Our model retrieves a paraphrase pair similar to the input sentence from a pre-defined index.
to create sentence representations. to create a non-parameterized approach for building sentence representations. We propose a simple and robust non-parameterized approach for building sentence representations. Inspired by the Gram-Schmidt Process in geometric theory, we build an orthogonal basis of the subspace spanned by a word and its surrounding context in a sentence.
and reranking hypotheses. VERNet achieves state-of-the-art grammatical error detection performance. VERNet achieves state-of-the-art grammatical error detection performance. VERNet achieves the best quality estimation results.
state tracking. to evaluate the relevance of dialogue history to the slot. This results in a dynamically selected dialogue content corresponding to each slot for state updating. Specifically, it first retrieves turn-level utterances of dialogue history and evaluates their relevance to the slot. Consequently, using consistent dialogue contents may lead to insufficient or redundant information for different slots.
in Transformer.s are effective to control the contributions from the source and target contexts in the recurrent neural network (RNN) based neural machine translation (NMT). However, it is challenging to extend them into the advanced Transformer architecture, which is more complicated than RNN. This paper provides a method to identify source and target contexts and then introduce a gate mechanism to control the source and target contributions in Transformer.
and evaluators. We compare human-based evaluators with a variety of automated evaluation procedures. We find that human evaluators do not correlate well with discriminative evaluators. We find that human decisions correlate better with discriminative evaluators.
, and to model them, we need probabilistic models of DAGs. We show that some DAG automata cannot be made into useful probabilistic models by the nearly universal strategy of assigning weights to transitions.
, we present the problem of cross-document RE, making an initial step towards knowledge acquisition in the wild. In this work, we present the problem of cross-document RE, making an initial step towards knowledge acquisition in the wild. To facilitate the research, we construct the first human-annotated cross-document RE dataset CodRED.
,,, and to predict their implied categories. We use pre-trained neural language models as linguistic knowledge sources for category understanding and representation learning models for document classification. Our method (1) associates semantically related words with the label names, (2) finds category-indicative words and trains the model to predict their implied categories.
s, and a failure to generate realistic sentences. To overcome these challenges, we propose the first end-to-end conditional generative architecture for generating paraphrases via adversarial training. Extensive experiments on four public datasets demonstrate the proposed method achieves state-of-the-art results.
, but worse in extraction quality. In this paper, we bridge this trade-off by presenting an iterative labeling-based system. OpenIE6 - beats the previous systems by as much as 4 pts in F1 over previous analyzers.
,,,,,: http://macchina-ai.eu/. Talk2Car dataset is the first object referral dataset that contains commands written in natural language for self-driving cars. Talk2Car is the first object referral dataset that contains commands written in natural language for self-driving cars.
and a human-understandable justification for the model’s predictions. We propose a modified LSTM cell with a diversity-driven training objective. Attention distributions offer more transparency as they (i) provide a more precise importance ranking of the hidden states (ii) are better indicative of words important for the model’s predictions (iii) correlate better with gradient-based attribution methods.
and a contextual model without using any additional data. We target the improvement of pronoun translations through our fine-tuning and evaluate our models on a pronoun benchmark testset.
adaptive training approaches can alleviate the token imbalance problem and thus improve neural machine translation. Token-level adaptive training approaches can alleviate the token imbalance problem and thus improve neural machine translation.
-based language model such as BERT has achieved great success in language understanding tasks. However, it remains an open question how to utilize BERT for language generation. In this paper, we present a novel approach, Conditional Masked Language Modeling (C-MLM), to enable the finetuning of BERT on target generation tasks. The proposed approach is exploited as extra supervision to improve conventional Seq2Seq models (student
, and IMoJIE, an extension to CopyAttention, which produces the next extraction conditioned on all previously extracted tuples. IMoJIE outperforms CopyAttention by about 18 F1 pts, and a BERT-based strong baseline by 2 F1 pts, establishing a new state of the art for the task.
sss. We then build individual graphs for each document and then use GNN to learn the fine-grained word representations based on their local structure. Extensive experiments on four benchmark datasets show that our method outperforms state-of-the-art text classification methods.
-based learning of VL tasks with a handful of examples and generalize to a new task without fine-tuning.We study prompt-based low-resource learning of VL tasks with our proposed method, FewVLM, relatively smaller than recent few-shot learners.Frozen is 31x larger than FewVLM by 18.2% point and achieve comparable results to a 246x larger model, PICa.
to predict semantic parse trees with an efficient seq2seq model architecture. By combining non-autoregressive prediction with convolutional neural networks, we achieve significant latency gains and parameter size reduction compared to traditional RNN models.
,, gSCAN, which is surprisingly data inefficient given the narrow scope of commands in gSCAN, suggesting another challenge for future work.
, data, and in a setting where in-domain labeled data is not available. SBERT is inefficient for sentence-pair tasks such as clustering or semantic search. SBERT is trained on corpus with high-quality labeled sentence pairs.
can be used to express certain types of messages. Languages typically provide more than one grammatical construction to express certain types of messages. A speaker’s choice of construction is known to depend on multiple factors, including the choice of main verb – a phenomenon known as verb bias. Here we introduce DAIS, a large benchmark dataset containing 50K human judgments for 5K distinct sentence pairs in the English dative alternation. This dataset includes 200 unique
and calibration and translation performance. We propose a new graduated label smoothing method that can improve inference calibration and translation performance.
,, and, and offer new strategies to break the data barrier. We review recent developments in and at the intersection of South Asian NLP and historical-comparative linguistics. We offer new strategies towards breaking the data barrier.
and supervised discourse information. supervised discourse trees with supervised, distantly supervised and simple baselines. We compare the captured discourse information with supervised, distantly supervised and simple baselines. We find that the captured discourse information is local and general, even across a collection of fine-tuning tasks.
,,,,,, and was 10x faster than the Monte Carlo approximation with 20000 samples on a common dataset.
-based hate, while retaining strong performance on text-only hate.
prediction is the task of finding the correct order of sentences in a randomly ordered document. Correctly ordering the sentences requires an understanding of coherence. Correctly ordering the sentences requires an understanding of coherence.
, we present a state-of-the-art model which is built on Vision-Language transformers. This paper explores the Navigation from Dialogue History (NDH) task, which is based on the Cooperative Vision-and-Dialogue Navigation (CVDN) dataset. despite achieving competitive performance, we find that the agent in the NDH task is not evaluated appropriately by the primary metric – Goal Progress.
, and,,,, and is competitive in the others, even with a simple linear prediction model. An ablation study shows how different data sets benefit from different aspects of our model.
sarcasm generation. We propose a novel framework for sarcasm generation. the system takes a literal negative opinion as input and translates it into a sarcastic version. Our framework does not require any paired data for training. Sarcasm emanates from context-incongruity which becomes apparent as the sentence unfolds.
0.05. We compute characterization scores among all 15 celebrities in our dataset. We use these classifiers to show surprisingly strong correlations between characterization scores and tweet popularity.
, and the context encoding is undertaken by contextual parameters, trained on document-level data. Multi-encoder models are broad family of context-aware neural machine translation systems. they aim to improve translation quality by encoding document-level contextual information alongside the current sentence.
, a wide spectrum of state-of-the-art PLMs and probing approaches on our benchmark, reaching at most 3% of acc@10. Contrastive-Probe is a novel self-supervised contrastive probing approach, that adjusts the underlying PLMs without using any probing data. Contrastive-Probe is a novel self-supervised contrastive probing approach, that adjusts the underlying
s.s. BLEU compared to maximum likelihood and scheduled sampling baselines. In addition, our approach is simpler to train with no need for sampling schedule and yields models that achieve larger improvements with smaller beam sizes.
. We propose a novel method that breaks up the limitation of these decoding orders, called Smart-Start decoding. We evaluate the proposed Smart-Start decoding method on three datasets. Experimental results show that the proposed method can significantly outperform strong baseline models.
– ADEPT – a large-scale semantic plausibility task consisting of over 16 thousand sentences. ADEPT – a large-scale semantic plausibility task consisting of over 16 thousand sentences.
classification tasks, useful information is encoded in the label names. Label semantic aware systems have leveraged this information for improved classification performance during fine-tuning and prediction. LSAP incorporates label semantics into pre-trained generative models (T5 in our case) by performing secondary pre-training on labeled sentences from a variety of domains.
,,,, and a distributional model collaborate seamlessly in cases which they each prefer. On several benchmark datasets, our framework demonstrates improvements that are both competitive and explainable.
, and we propose an annotated dataset called Para-Quality, for detecting the quality issues. In this paper, we investigate common crowdsourced paraphrasing issues, and propose an annotated dataset called Para-Quality, for detecting the quality issues.
based on learninger English dataset. This dataset contains manually annotated error causes for learner writing errors. These causes tie learner mistakes to structures from their first languages, when the rules in English and in the first language diverge. This new dataset can also be applied in personalizing grammatical error correction systems according to the learners’ first language and in providing feedback that is informed by the cause of an error.
detection, which aims to verify whether a news document is trusted or fake, has become urgent and important. In this paper, we propose a novel end-to-end graph neural model called CompareNet. Considering that fake news detection is correlated with topics, we also incorporate topics to enrich the news representation.
,,,,,-based games present a unique challenge for autonomous agents to operate in natural language and handle enormous action spaces. Text-based games present a unique challenge for autonomous agents to operate in natural language. In this paper, we propose the Contextual Action Language Model (CALM) to generate a compact set of action candidates at each game state.
, we hope to increase attention on the inaccurate evaluation of attribution scores.
, the contexts where the answers appear. This process also gave many questions without answers, and those that require discrete reasoning, increasing the difficulty of the task. To fill this gap, we present a dataset, IIRC, with more than 13K questions over paragraphs from English Wikipedia that provide only partial information to answer them.
, we rank and filter motif instances to distill highly label-indicative ones as “seed motifs”. Using seed words, we rank and filter motif instances to distill highly label-indicative ones as “seed motifs”.
. The Transformer-based model is widely equipped in the recent abstractive summarization models. In this work, we propose a Transformer-based model to enhance the copy mechanism. We identify the importance of each source word based on the degree centrality with a directed graph built by the self-attention layer in the Transformer.
aims to provide lightweight unified analysis for all languages at the word level. The proposed annotation scheme is conceptual promising, but the feasibility is only examined in four Indo–European languages.
,,,,s-based neural topic model that interprets topics as mixtures of words and phrases by performing a nearest neighbor search in the embedding space. In this paper, we propose a contrastive fine-tuning objective that enables BERT to produce more powerful phrase embeddings.
(MT) systems perform remarkably well on clean, in-domain text. However most of the human generated text is full of typos, slang, dialect, idiolect and other noise which can have disastrous impact on the accuracy of MT. In this paper we propose methods to enhance the robustness of MT systems by emulating naturally occurring noise in otherwise clean data.
, gender and language around it, and detail how current language representations (e.g. GloVe, BERT) capture and perpetuate these harms and related challenges that need to be acknowledged and addressed.
and, we show that on the GLUE tasks, the combination of our pretrained CNN with ALBERT achieved stronger performance than SOTA and the original TLMs. QA (Quasar-T and SearchQA), the combination of our pretrained CNN with ALBERT achieved stronger performance than SOTA and the original TLMs.
,,,, XLM (Lample and Conneau, 2019), a recently proposed cross-lingual language model trained with massive parallel data, and achieve highly competitive results.
a machine-augmented human attention supervision.,,,, in text classification tasks, including sentiment analysis and news categorization.. leverage human and machine intelligence together for attention supervision. Specifically, we show that human annotation cost can be kept reasonably low, while its quality can be enhanced by machine self-supervision.
in the pre-training and fine-tuning stages. This results show that our model achieves state-of-the-art performance on six public benchmark datasets.
and decoder incompatibility, whose optimization usually converges into a trivial local optimum termed posterior collapse. We argue that the trivial local optimum may be avoided by improving the encoder and decoder parameterizations. We propose Coupled-VAE, which couples a VAE model with a deterministic autoencoder with the same structure.
the algorithm to filter out doubtful predictions. In the second case, we propose a weakly supervised method for combining the predictions into a final one.
for text generation. GANs are promising approaches for text generation that, unlike traditional language models (LM), does not suffer from the problem of “exposure bias”. However, a major hurdle for understanding the potential of GANs for text generation is the lack of a clear evaluation metric. In this work, we propose to approximate the distribution of text generated by a GAN, which permits evaluating them with traditional probability-based LM metrics
, based on the sequence-to-sequence paradigm. The topic-aware parameterization directly generates the parameters by capturing local semantics of the given context. The topic-aware parameterization enables parameter sharing among conversations with similar topics by inferring the latent topics of the given context.
technique which uses k-fold cross-validation across multiple data sets to estimate the likelihood that one model will outperform the other, or that the two will produce practically equivalent results.
. We present how to utilize subword language models for the fast and accurate generation of query completion candidates. Using subword language models, we develop a retrace algorithm and a reranking method by approximate marginalization.
and low disparities. We explore the implications of this phenomenon for model fairness across demographic groups in clinical prediction tasks over electronic health records (EHR) in MIMIC-III —— the standard dataset in clinical NLP research. We find that jointly optimizing for high overall performance and low disparities does not yield statistically significant improvements.
in a pre-trained word vector space. To this end, we made several assumptions about the distribution, modeled the distribution accordingly, and validated each assumption by comparing the goodness of each model. We further investigated the relationship between the scores calculated by each model and the degree of membership.
detection. We propose a novel task of cross-document misinformation detection. Given a cluster of topically related news documents, we aim to detect misinformation at both document level and a more fine-grained level, event level.
detection. detection. We conduct experiments on Propaganda Techniques Corpus, a large manually annotated dataset for fine-grained propaganda detection. We conduct experiments on Propaganda Techniques Corpus, a large manually annotated dataset for fine-grained propaganda detection.
are sensitive to small perturbations in the input. Our proposed metrics reveal a clear trend of improved robustness to perturbations when subword regularization methods are used. Results show that our proposed metrics reveal a clear trend of improved robustness to perturbations when subword regularization methods are used.
detection in loan applications....,, and achieve higher recognition accuracy compared with rule-based systems. Our learned dialogue strategies are interpretable and flexible, which can help promote real-world applications.
, but not syntactic structure. Tree structures have shown strong performance in tasks with sentence pair input like semantic relatedness. We introduce the Tree Aggregation Transformer: a novel recursive, tree-structured self-attention model for AS2.
,,,, and.. henryzhao5852/BeamDR. complex question answering often requires finding a reasoning chain that is multiple evidence pieces. retrieval methods often require finding a reasoning chain that is multi-hop. We propose a new multi-step retrieval approach (BeamDR) that iteratively forms an evidence chain through beam search in dense representations.
for visual dialog. We develop CLEVR-Dialog, a large diagnostic dataset for studying multi-round reasoning in visual dialog. CLEVR-Dialog contains 5 instances of 10-round dialogs for about 85k CLEVR images, totaling to 4.25M question-answer pairs.
a weakly-supervised method for training deep learning models for the task of document retrieval. We describe a weakly-supervised method for training deep learning models for the task of ad-hoc document retrieval. Our method is based on generative and discriminative models that are trained using weak-supervision just from the documents in the corpus.
, which is demonstrated with strong language modeling capability, however, is not personalized. To address this problem, we present a PErsonalized Transformer for Explainable Recommendation (PETER) on which we design a simple and effective learning objective that uses the IDs to predict the words in the target explanation. Extensive experiments show that our small unpretrained model outperforms fine-tuned BERT on the generation
and, XLNet achieves a Macro F1 score of only 78.18% and an accuracy of 78.23%. SciNLI is harder to classify than the existing NLI datasets.
,,, and, and, and our model is comprised of two-level attention (word/object and frame level), multi-head self/cross-integration for different sources (video and dense captions) and gates which pass more relevant information to the classifier. In this paper, we propose a video question answering model which effectively integrates multi-modal input sources and finds the temporally relevant information to answer questions.
,, and neural encoder–decoder models., and have come to different conclusions., and using different evaluation methods., and have come to different conclusions., and using different evaluation methods. modern approach to historical text normalization.-of-the-art approach to historical text normalization. approach to modern modernization. approach to modernization. modernization. modernization.
to diagnose the ‘I don’t know’ problem, in which a dialog system produces generic responses. RUQ allows for the direct analysis of the ‘I don’t know’ problem, which has been addressed but not analyzed by prior work.
in the SCAN domain, it boosts accuracies from 14.0% to 98.8% in Jump task, and from 92.0% to 99.7% in TurnLeft task. It beats human performance on a few-shot learning task.
Adaptation to train a Language Discriminator to discern between the source and target languages using unlabeled data. We optimize the adversarial training process by only presenting the discriminator with the most informative samples.
and, and scales to large models (e.g., BERT) for NLP applications. We propose Dirichlet Neighborhood Ensemble (DNE), a randomized method for training a robust model to defense synonym substitution-based attacks. DNE is robust to adversarial attacks while maintaining the performance on the original clean data.
of word group membership, and can be applied to a number of tasks reasoning over the SVO structure. We present a neural framework for learning associations between interrelated groups of words such as the ones found in Subject-Verb-Object (SVO) structures. Our model induces a joint function-specific word vector space, where vectors of e.g. plausible SVO compositions lie close together.
, we use a scope detector to introduce the scope of negation as an additional input to the network. Experimental results show that doing so obtains the best results to date. Additionally, we perform a detailed error analysis providing insights into the main error categories.
, and the effectiveness of our proposed NPD approach in capturing such personal attributes with significant gains over the state-of-the-art models.
,,, and edges to represent natural transition between a message (the last utterance in a dialog context) and its response. We present a novel CG grounded policy learning framework that conducts dialog flow planning by graph traversal. CG is a new CG grounded policy learning framework that conducts dialog flow planning by graph traversal, in which there are vertices to represent “what to say” and “how to say”
, which is 2.9M pairs larger than the benchmark Vietnamese-English machine translation corpus IWSLT15. We introduce a high-quality and large-scale Vietnamese-English parallel dataset of 3.02M sentence pairs. We conduct experiments comparing strong neural baselines and well-known automatic translation engines on our dataset.
and RP are highly correlated and usually employed jointly in real-world e-commerce scenarios. We propose an intuitive yet effective joint model for ACSA and RP.
s, and,, and paired sentences. We present Polyjuice, a general-purpose counterfactual generator that allows for control over perturbation types and locations. Polyjuice produces diverse sets of realistic counterfactuals, which are useful in various distinct applications.
generation model with paired and unpaired data. We propose leveraging the large scale of unpaired data that are much easier to obtain, and propose response generation with both paired and unpaired data.
s, and TEMP, a self-supervised taxonomy expansion method, which predicts the position of new concepts by ranking the generated taxonomy-paths. Extensive evaluations show that TEMP outperforms prior state-of-the-art taxonomy expansion approaches by 14.3% in accuracy and 15.8% in mean reciprocal rank on three public benchmarks.
, and,, and a 50% relative improvement in human evaluations.,, and a 50% relative improvement in human evaluations. model that successfully collaborates with people in a partially-observable reference game. We present a grounded neural dialogue model that successfully collaborates with people in a partially-observable reference game. We present a grounded neural dialogue model that successfully collaborates with people in a partially-observable reference game
,,,,ing space bidirectionally will bring significant improvement. We follow the previous iterative framework to conduct experiments. Results on standard benchmark demonstrate the effectiveness of our proposed method, which substantially outperforms previous unsupervised methods.
,,,, hobbies, favorite food, and travel preferences, among others, is a valuable asset for individualized AI, such as recommenders or chatbots. This paper overcomes this limitation by devising CHARM: a zero-shot learning method that creatively leverages keyword extraction and document retrieval in order to predict attribute values that were never seen during training.
. We explore five experimental methods inspired by prior work evaluating pretrained sentence representation models. We use a single linguistic phenomenon, negative polarity item (NPI) licensing, as a case study for our experiments.
analysis aims to determine the sentiment polarity towards a specific aspect in online reviews. Firstly, we define a unified aspect-oriented dependency tree structure rooted at a target aspect by reshaping and pruning an ordinary dependency parse tree. then, we propose a relational graph attention network (R-GAT) to encode the new tree structure for sentiment prediction.
s, we compute each input step efficiently in parallel, avoiding the formerly costly sequential linear transformations. To enable sequence-level parallelization of LSTMs, we approximate full LSTM context modelling by computing hidden states and gates with the current input and a simple bag-of-words representation of the preceding tokens context. To further constrain the number of LSTM parameters, we compute several small HPLSTMs in parallel.
rewrites a source text in a different target style while preserving its content. Style transfer aims to rewrite a source text in a different target style while preserving its content.
,, and the syntactic dependency relations between words in a question. Visual question answering aims to answer the natural language question about a given image. Existing graph-based methods only focus on the relations between objects in an image and neglect the importance of the syntactic dependency relations between words in a question.
,, and,, and multiple keyphrase-relevant headlines. We propose a multi-source Transformer decoder, which takes three sources as inputs: (a) keyphrase, (b) keyphrase-filtered article, and (c) original article to generate keyphrase-relevant headlines. We propose a simple and effective method to mine the keyphrases of interest in the news article and build a first large-scale keyphrase-
recognition (NER) has received increasing attention. The majority of previous work focuses on overlapped and discontinuous entities. In this paper, we propose a novel span-based model that can recognize both overlapped and discontinuous entities jointly.
in many supervised NLP tasks. We propose UXLA, a novel unsupervised data augmentation framework for zero-resource transfer learning scenarios. UXLA achieves SoTA results in all the tasks, outperforming the baselines by a good margin.
s for pretrained weights in lieu of modifying them through finetuning. We present an efficient method of utilizing pretrained language models, where we learn selective binary masks for pretrained weights.
. In this context, we propose a new competitive mechanism that encourages these attention heads to model different dependency relations. We introduce a new model, the Unsupervised Dependency Graph Network (UDGN), that can induce dependency structures from raw corpora and the masked language modeling task.
,,,, and WikiText-103, a language modeling dataset. The experiments show that PermuteFormer uniformly improves Performer performance. PermuteFormer introduces negligible computational overhead by design that it runs as fast as Performer.
s in different languages. We evaluate our models on the Rationalized English-French Semantic Divergences, a new dataset released with this work. Learning to rank helps detect fine-grained divergent examples of varying granularity. Learning to rank helps detect fine-grained semantic divergences more accurately than a strong sentence-level similarity model.
s (GANs) for text generation suffer from the instability of reinforcement learning training algorithms such as policy gradient. To tackle this problem, we propose a novel framework called Adversarial Reward Augmented Maximum Likelihood (ARAML) to tackle this problem.
and boosting to identify large-error instances and discover new labeling rules from data. PRBoost outperforms state-of-the-art WSL baselines up to 7.1%.
ization (CLS) is the task to produce a summary in one particular language for a source document in a different language. Existing methods simply divide this task into two steps: summarization and translation, leading to the problem of error propagation. To handle that, we present an end-to-end CLS framework, which we refer to as Neural Cross-Lingual Summarization (NCLS), for the first time.
and the propagation structure. Moreover, we design a new edge-wise consistency training framework to optimize the model by enforcing consistency on relations. Toward this issue, this paper makes the first attempt to explore propagation uncertainty for rumor detection.
s, we present Virtual Data Augmentation (VDA), a general framework for robustly fine-tuning PLMs. Our codes and data are publicly available at bluehttps://github.com/RUCAIBox/VDA.
s ands. We propose and evaluate classifiers to determine if a particular pair of entities are allies or enemies. We find that Wikipedia articles of allies are semantically more similar than enemies.
, and that fill important roles in a coherent story. We propose a new Event Graph Schema, where two event types are connected through multiple paths involving entities that fill important roles in a coherent story. We then introduce Path Language Model, an auto-regressive language model trained on event-event paths. We then introduce Path Language Model, an auto-regressive language model trained on event-event paths.
,,,, for each word in the sentences. We perform extensive experiments to demonstrate the effectiveness of the proposed method, leading to the state-of-the-art performance on three RE benchmark datasets.
show that a general algorithm for efficient computation of outside values under the minimum of superior functions framework proposed by Knuth (1977) would yield a sub-exponential time algorithm for SAT, violating the Strong Exponential Time Hypothesis (SETH).
span QA. We propose a span-based QA framework (QGH) to address NLVL.
elicits the gold label, and models can assign gold labels to permutations. This is a phenomenon that exists in pre-Transformer RNN / ConvNet based encoders.
,,,,, and, and the application of such measures. Moreover, we urge researchers to be more critical about the application of such measures. Moreover, we urge researchers to be more critical about the application of such measures.
, and a quadratic weighted kappa of 0.6 is obtained. We use this dataset to train a model that provides a ‘funniness’ score, on a five-point scale (0-4). We use this dataset to train a model that provides a ‘funniness’ score, on a five-point scale (0-4).
,, REM and CEM to control data generation. EPiDA can support effective text classification. EPiDA can support efficient and continuous data generation for effective classifier training. Extensive experiments show that EPiDA outperforms existing SOTA methods in most cases, though not using any agent networks or pre-trained generation networks. EPiDA can support efficient and continuous data generation for effective classifier training.
, and NMT, which achieve strong performance in image generation due to their invertibility and exact density estimation properties, have been less explored for NLG. In this paper, we propose a flow-based language generation model by adapting previous flow generative models to language generation.
, which is notorious for being slow in inference, which makes them difficult to deploy in real-time applications. We propose a simple but effective method, DeeBERT, to accelerate BERT inference. Our approach allows samples to exit earlier without passing through the entire model. Further analyses show different behaviors in the BERT transformer layers and also reveal their redundancy.
, and a Macro F1 of 49,2%, showing that this is a challenging testbed for claim veracity prediction.
, and that using a mixture model resulting from interpolating control and dementia LMs improves upon the current state-of-the-art.
-based scores that express the confidence with which each frame applies to the word. We present a resource for the task of FrameNet semantic frame disambiguation of over 5,000 word-sentence pairs from the Wikipedia corpus.
relation extraction using generative adversarial network framework (GAN) framework. To address this issue, we propose a novel semi-distant supervision approach for relation extraction.
summarization is the task of writing short, natural language descriptions of source code. The main use for these descriptions is in software documentation e.g. the one-sentence Java method descriptions in JavaDocs. Code summarization is the task of writing short, natural language descriptions of source code.
generation. We pre-train the model using the following 4 tasks, used in training language models (LMs) and VAEs literature: 1) masked language model; 2) response generation; 3) bag-of-words prediction; and 4) KL divergence reduction. We also add additional parameters to model the turn structure in dialogs to improve the performance of the model.
, and 2) a novel dataset of region-labeled articles. A continuing challenge in scientific literature mining is the difficulty of consistently extracting high-quality text from formatted PDFs. We present 1) a work in progress method to visually segment key regions of scientific articles using an object detection technique augmented with contextual features, and 2) a novel dataset of region-labeled articles.
MT and MT systems. We propose two partial remedies: (1) post-hoc re-alignment of the vector spaces and (2) coupling of semantic-similarity based metrics with target-side language modeling.
and multilingual pretrained transformer models to predict reading time measures reflecting natural human sentence processing on Dutch, English, German, and Russian texts. This results in accurate models of human reading behavior. We analyze if large language models are able to predict patterns of human reading behavior. We compare the performance of language-specific and multilingual pretrained transformer models to predict reading time measures reflecting natural human sentence processing on Dutch, English, German, and Russian texts. This results in accurate models of
against women at a comment level and present a model that can surface text likely to contain bias. Our main challenge is forcing the model to focus on signs of implicit bias, rather than other artifacts in the data. Ultimately, our work offers a way to capture subtle biases in various domains without relying on subjective human judgements.
,,, which is geared towards simple understanding of one-turn queries. We release a new session-based, compositional task-oriented parsing dataset of 20k sessions consisting of 60k utterances. We propose a new family of Seq2Seq models for the session-based parsing above, which also set state-of-the-art in ATIS, SNIPS, TOP and DSTC
to achieve the alignment in a single-step process. This paper proposes an Iterative Alignment Network (IA-Net) for TSG task. This work focuses on how to learn effective alignment between vision and language features.
on text from the target domain. We propose domain-adaptive fine-tuning, in which contextualized embeddings are adapted by masked language modeling on text from the target domain.
,, the evaluation setting and the dataset statistics. We call for unifying the evaluation setting in end-to-end RE.
ssss, but are not representative of natural language variation. We propose new train and test splits of non-synthetic datasets. We demonstrate that strong existing approaches do not perform well across a broad set of evaluations. We also propose NQG-T5, a hybrid model that combines a high-precision grammar-based approach with a pre-trained sequence-to-sequence model.
models improve performance. The results show differences in ability to represent compositional and lexical information between RoBERTa, BERT and DistilBERT.
structure. We demonstrate a significant improvement on two benchmark entity coreference-resolution datasets.
deficiency is one of the main limiting factors of word representations which, given their widespread use at the core of many NLP systems, can lead to inaccurate semantic understanding of the input text. Meaning conflation deficiency is one of the main limiting factors of word representations which, given their widespread use at the core of many NLP systems, can lead to inaccurate semantic understanding of the input text.
,,,,,, with significant improvement in accuracy and efficiency. VisualSparta is the first transformer-based text-to-image retrieval model that can achieve real-time searching for large-scale datasets. VisualSparta is the first transformer-based text-to-image retrieval model that can achieve real-time searching for large-scale datasets.
coordinate structures. Experimental results demonstrate that our model achieves state-of-the-art results, ensuring that the global structure of coordinations is consistent.
, a graph module network built upon the Transformer-based architecture. LogicalFactChecker is a neural network approach capable of leveraging logical operations for fact checking. This is achieved by a graph module network built upon the Transformer-based architecture. LogicalFactChecker automatically derives a textual statement with semi-structured tables.
’s homophony is a controversial topic. Recent theories of language optimality have tried to justify its prevalence. Trott and Bergen (2020) argued that good wordforms are more often homophonous simply because they are more phonotactically probable.
, a counterfactual multi-granularity graph supporting facts extraction (CMGE) method to extract supporting facts from irregular EMR itself without external knowledge bases. Using counterfactual multi-granularity graph supporting facts extraction (CMGE) method, we extract supporting facts from irregular EMR itself without external knowledge bases. Using the same method, we can diagnose four types of EMR correctly, and can provide accurate supporting facts for the diagnosis.
,,, which can be seamlessly integrated into NDR models to perform the imagination of unseen counterfactual. In this work, we devise a Learning to Imagine module, which can be seamlessly integrated into NDR models to perform the imagination of unseen counterfactual. In this work, we devise a Learning to Imagine module, which can be seamlessly integrated into NDR models to perform the imagination of unseen counterfactual
and applications, and presents a third research paradigm, focused on understanding the challenges posed by application needs. Translational NLP presents a third research paradigm, focused on understanding the challenges posed by application needs. Translational NLP presents a third research paradigm, focused on understanding the challenges posed by application needs.
ands-based language models. Our method increases the performance by an improvement rate of 20,8% over the zero-shot transfer learning from BERT.
,,,,, and other desirable properties, e.g., they generate text that is more lexically diverse. Our results suggest that UID is a reasonable inductive bias for language modeling.
,, and, and improve the compactness of representations. We propose to fine-tune the Transformers with a contrastive loss, which improves the compactness of representations. We propose to fine-tune the Transformers with a contrastive loss, which improves the compactness of representations.
pre-training has led to much recent progress in natural language understanding. In this paper, we study self-training as another way to leverage unlabeled data through semi-supervised learning.
, and BERT-MLMs are a powerful tool for biomedical text classification. We propose BBAEG (Biomedical BERT-based Adversarial Example Generation), a black-box attack algorithm for biomedical text classification. BBAEG (Biomedical BERT-based Adversarial Example Generation), a black-box attack algorithm for biomedical text classification.
(IE) aims to extract structural information from unstructured texts. In practice, long-tailed distributions caused by the selection bias of a dataset may lead to incorrect correlations. This motivates us to propose counterfactual IE (CFIE), a novel framework that aims to uncover the main causalities behind data in the view of causal inference.
to the new domain. We propose a cross-lingual data selection method to extract in-domain sentences in the missing language side from a large generic monolingual corpus. Our proposed method trains an adaptive layer on top of multilingual BERT by contrastive learning to align the representation between the source and target language. This then enables the transferability of the domain classifier between the languages in a zero-shot manner.
, a well-developed pre-training scheme. We propose the training objective MiSAD that uses meaningful n-grams extracted from large unlabeled corpus. We then empirically verify that well designed pre-training scheme may effectively yield universal language representation. Especially, our model achieves the highest accuracy on analogy tasks in different language levels and significantly improves the performance on downstream tasks in the GLUE benchmark and a question answering dataset
extraction, DocEE, a new document-level event extraction dataset including 27,000+ events, 180,000+ arguments. DocEE is now available at https://github.com/tongmeihan1995/DocEE.git.
s.s.s.s.s.s.s.s. We propose a novel model CAST that hierarchically splits and reconstructs ASTs. First, we hierarchically split a large AST into a set of subtrees. Then, we aggregate the embeddings of subtrees by reconstructing the split ASTs. Finally, we aggregate the embedding
using noisy labels from multiple weak supervision sources. To address this challenge, we propose a hidden Markov model (CHMM) which can effectively infer true labels from noisy labels in an unsupervised way.
,, reranking and generation, with relative gains of 9% to 34% over the previous state-of-the-art on the KILT leaderboard.
ss-time masking techniques.-based summarization models.-yet-effective attention head masking technique.-yet-effective attention head masking technique.-yet-effective attention head masking technique..,s.-based abstractive summarization models?-yet-effective attention head masking technique.-based abstractive summarization models.
, and we propose to answer open-domain multi-answer questions with a recall-then-verify framework. rerank-then-read is a rerank-then-read framework, where a reader reads top-ranking evidence to predict answers. reranker should select only a few relevant passages to cover diverse answers, while balancing relevance and diversity is non-trivial.
, and a new state-of-the-art with improvements of 4.4% point joint goal accuracy on average across domains. Labeler generates dialogues from (1) user’s goal instructions, which are the user context and task constraints in natural language. Labeler generates dialogues from (1) user’s goal instructions, which are the user context and task constraints in natural language.
, BITFIT, LNFIT, originally proposed to accelerate fine-tuning for new end-tasks, help retain multilingual knowledge from pretraining, substantially improving zero-shot cross-lingual transfer. Adaptation strategies, namely partial fine-tuning, adapters, BITFIT, LNFIT, originally proposed to accelerate fine-tuning for new end-tasks, help retain multilingual knowledge from pretraining, substantially improving zero
and generation. We propose a neural co-generation model that generates fluent and informative responses is of critical importance for task-oriented dialogue systems. Existing pipeline approaches generally predict multiple dialogue acts first and use them to assist response generation. Second, the semantic associations between acts and responses are not taken into account for response generation.
tagging of unigram and bigram data.
ization aims to generate a short summary for an input text. In this work, we propose a Non-Autoregressive Unsupervised Summarization (NAUS) approach. NAUS performs edit-based search towards a heuristically defined score, and generates a summary as pseudo-groundtruth.
, which is a central tool for measuring progress in natural language understanding. This position paper describes and critiques the Pretraining-Agnostic Identically Distributed (PAID) evaluation paradigm, which has become a central tool for measuring progress in natural language understanding. This paradigm favors simple, low-bias architectures, which, first, can be scaled to process vast amounts of data, and second, can capture the fine-grained statistical properties of
,, and reference texts. AVA is an automatic evaluation approach for Question Answering, which gives questions associated with Gold Standard answers (references) can estimate system Accuracy. AVA uses Transformer-based language models to encode question, answer, and reference texts. This allows for effectively assessing answer correctness using similarity between the reference and an automatic answer, biased towards the question semantics.
,,, and machine translation-based baselines. This appears to be the first study that transfers feature domains in detecting cognitive decline.
– toxicity prediction and sentiment analysis.
ACE2005 named entity recognition benchmark. Besides, incorporating gazetteer network can further improve the performance and significantly reduce the requirement of training data.
for CPTP, we propose the Deep Gating Network (DGN) for charge-specific feature selection and aggregation. Experiments show that DGN achieves the state-of-the-art performance.
,, and human effort. We introduce a human-machine collaborative framework, HMCEval, that can guarantee reliability of the evaluation outcomes with reduced human effort. HMCEval achieves around 99% evaluation accuracy with half of the human effort spared.
anaphora is rarer and more complex to resolve than single-antecedent anaphora. Split-antecedent anaphora is rarer and more complex to resolve than single-antecedent anaphora. this is not annotated in many datasets designed to test coreference.
in sequence-to-sequence models. We introduce a jointly masked sequence-to-sequence model and explore its application on non-autoregressive neural machine translation(NAT)(NAT)
a cross-lingual summary. We first attend to some words in the source text, then translate them into the target language. We present three strategies to acquire the translation probability, which helps obtain the translation candidates for each source word.
, but it is unsurprisingly small (less than 0.5% on average according to our information-theoretic estimate). This work presents an information-theoretic operationalisation of cross-linguistic non-arbitrariness. It is not a new idea that there are small, cross-linguistic associations between the forms and meanings of words.
based on deep contextualized embeddings (e.g., BERT, Roberta, ELMo) is introduced. This metric is motivated by a new framework relying on optimal transport tools. This metric is based on a new metric baryScore to evaluate text generation based on deep contextualized embeddings (e.g., BERT, Roberta, ELMo) is introduced.
,, and, and identifying the argumentative propositions submitted by reviewers. We then train state-of-the-art proposition segmentation and classification models on the data to evaluate their utilities and identify new challenges for this new domain.
, Pointer, Disambiguator, and Copier. The method PDC achieves the following merits inherently compared with previous efforts: (1) Pointer leverages semantic information from bilingual dictionaries, for the first time, to better locate source words whose translation in dictionaries can potentially be used; (2) Disambiguator synthesizes contextual information from the source view and the target view, both of which contribute to distinguishing the proper translation of a specific
, multilingual T5 pretrains a sequence-to-sequence model on massive monolingual texts. Multilingual T5 pretrains a sequence-to-sequence model on massive monolingual texts. Multilingual T5 pretrains a sequence-to-sequence model on massive monolingual texts.
, we propose an efficient KD paradigm to compress BERT, which does not require loading the teacher during the training of student.
,, are among the most performant methods for solving sequence labeling problems. Masked Conditional Random Field (MCRF) based neural models are among the most performant methods for solving sequence labeling problems. Masked Conditional Random Field (MCRF) based neural models are among most performant methods for solving sequence labeling problems.
, we propose a new approach to human evaluation and identify several challenges that must be overcome to develop effective biomedical MDS systems.
and data from commit histories of open-source software projects. We train and evaluate our model using a dataset we collected from commit histories of open-source software projects. Results reflect the challenge of this task and that our model outperforms baselines with respect to making edits.
in the cognitive neuroscience and psycholinguistics communities as a tool to study language comprehension. characterization of each individual ERP in terms of what features of a stream of language trigger the response remains controversial. Improving this characterization would make ERPs a more useful tool for studying language comprehension.
in contextualized embedding could help explain their impressive performance across NLP. We develop a heuristic, DirectProbe, that directly studies the geometry of a representation by building upon the notion of a version space for a task.
of the same side stance classification (S3C) has been proposed. To ease the difficulty of argument stance classification, the task of same side stance classification (S3C) has been proposed.
ssizationization and dialoggeneration. Wedemonstrate that these errors can be mitigatedby explicitly designing evaluation metrics toavoid spurious features in reference-free evaluation.
,, and.,, and does not rely entirely on becoming more extractive to improve factuality. FactPEGASUS is more factual than using the original pre-training objective in zero-shot and few-shot settings. FactPEGASUS is more factual than using the original pre-training objective in zero-shot and few-shot settings. FactPEGASUS is more factual than using the original pre-training objective
resolution is an important compo-nent in analyzing narrative text from admin-istrative data (e.g., clinical or police sources). Coreference resolution is an important compo-nent in analyzing narrative text from admin-istrative data (e.g., clinical or police sources). Coreference resolution is an important compo-nent in analyzing narrative text from admin-istrative data (e.g., clinical or police
utterances to initiate conversation without relying on boilerplate utterances like greetings. To address the lack of training data for this task, we constructed a novel large-scale dataset through crowd-sourcing.
s in word meanings (hereafter, semantic variations) with personalized word embeddings obtained by solving a task on subjective text while describing words used by different individuals as different words.
, ReGen, a bidirectional generation of text and graph leveraging Reinforcement Learning to improve performance. ReGen is a bidirectional generation of text and graph leveraging Reinforcement Learning to improve performance. Graph linearization enables us to re-frame both tasks as a sequence to sequence generation problem regardless of the generative direction.
:
pre-training (SEQ2SEQ) objectives which allow us to pre-train a SEQ2SEQ based abstractive summarization model on unlabeled text. Using supervised summarization data, a model is pre-trained to reinstate the original document. compared to models pre-trained on large-scale data (larger than 160GB), our method, with only 19GB text for pre-training,
for semantic matching (CNM) achieves comparable performances to strong CNN and RNN baselines on two benchmarking question answering (QA) datasets.
, as the presence of constituents with discontinuous yield introduces extra complexity to the task. To that end, we propose to reduce discontinuous parsing to a continuous problem, which can be directly solved by any off-the-shelf continuous parser.
,,,, and, and claim verification., and document retrieval. a task-agnostic pipelined system, AttentiveChecker.-agnostic pipelined system, AttentiveChecker.-agnostic pipelined system, AttentiveChecker.-agnostic pipelined system,-agnostic pipelined system,.,.
and,, and, and their combination shows further improvement. latent variable models suffer from a huge gap between prior and posterior knowledge selection. Knowledge selection plays an important role in knowledge-grounded dialogue, which is a challenging task to generate more informative responses by leveraging external knowledge.
, despite performing two tasks (SU segmentation and parsing) rather than one (parsing alone). Proody can effectively replace gold standard SU boundaries: with prosody, the turn-based model performs as well as the SU-based model (91.38 vs. 91.06 F1 score, respectively) despite performing two tasks (SU segmentation and parsing) rather than one (parsing alone).
ssssss andsity is an outstanding challenge for entity alignment. Structural heterogeneity between knowledge graphs is an outstanding challenge for entity alignment. This paper presents Neighborhood Matching Network (NMN), a novel entity alignment framework for tackling the structural heterogeneity challenge. NMN estimates the similarities between entities to capture both the topological structure and the neighborhood difference.
,, and whether neurons process subject-verb agreement similarly across sentences with different syntactic structures. We uncover similarities and differences across architectures and model sizes—notably, that larger models do not necessarily learn stronger preferences.
,,,,,, allowing us to apply our automatic perturbation to the training set to mitigate the degradation in performance. We present a novel method which leverages rich semantic input representation to automatically generate contrast sets for the visual question answering task. Contrast sets (Gardneret al., 2020) quantify this phenomenon by perturbing test samples in a minimal way such that the output label is modified.
based on fact-level content weighting, i.e. relating the facts of the document to the facts of the summary. We introduce a new evaluation metric which is based on fact-level content weighting. i.e. relating the facts of the document to the facts of the summary.
for Grammatical Error Correction (GEC) task. In this paper, we propose a copy-augmented architecture for the GEC task by copying the unchanged words from the source sentence to the target sentence.
generation, where a shared memory is designed to record the alignment between images and texts so as to facilitate the interaction and generation across modalities. Experimental results illustrate the effectiveness of our proposed model, where state-of-the-art performance is achieved on two widely used benchmark datasets, i.e., IU X-Ray and MIMIC-CXR.
,,,,,, and how word order encodes meaning essential to performing these tasks. We refute this hypothesis in many cases: in the GLUE suite and in various genres of English text, the words in a sentence or phrase can rarely be permuted to form a phrase carrying substantially different information. Our surprising result relies on inference by iterative shuffling (IBIS), a novel, efficient
-based table question-answering (TableQA) models have achieved state-of-art performance by using pre-trained BERT transformer. TableQA models have achieved state-of-art performance by using pre-trained BERT transformer to jointly encoding a question and a table to produce structured query for the question. TableQA systems are deployed over table corpora having topic and word distributions quite distinct from BERT’s pretraining
and analysis, and we create two challenge datasets to evaluate model performance on negated and speculative samples. We find that multitask models and transfer learning via language modelling can improve performance on these challenge datasets, but the overall performances indicate that there is still much room for improvement.
to interpretable reasoning for complex questions. We propose a novel mechanism to enable conventional KV-MemNNs models to perform interpretable reasoning for complex questions.
the novel PANACEA dataset consisting of heterogeneous claims on COVID-19 and their respective information sources. We present a comprehensive work on automated veracity assessment from dataset creation to developing novel methods based on Natural Language Inference (NLI) focusing on misinformation related to the COVID-19 pandemic.
inginginginging-domain datasets, outperforming several existing and employed strong baselines.
s, we show that state-of-the-art models (but not a relatively strong baseline) can learn the function subtending the meaning of size adjectives. gradable adjectives of size (‘big’, ‘small’) can be learned from visually-grounded contexts.
of language encoders (RSA) is a technique developed by neuroscientists for comparing activity patterns of different measurement modalities. RSA is a technique developed by neuroscientists for comparing activity patterns of different measurement modalities (e.g., fMRI, electrophysiology, behavior). RSA has several advantages over existing approaches to interpretation of language encoders based on probing or diagnostic classification.
rewrite and query rewrite. We propose a novel joint learning framework of modeling coreference resolution and query rewriting. Traditionally, anaphora is resolved by coreference resolution and ellipses by query rewrite.
,, and,,, and that they follow the same statistical linguistics laws., model RevGAN. model RevGAN that automatically generates controllable and personalized user reviews based on the arbitrarily given sentimental and stylistic information. model RevGAN. model RevGAN reviews. laws.
, and we introduce an iterative method to extract code idioms from large source code corpora. In this paper, we introduce an iterative method to extract code idioms from large source code corpora. Programmers typically organize executable source code using high-level coding patterns or idiomatic structures.
, we use the phylogenetic tree to guide the learning of multi-lingual dependency parsers leveraging languages structural similarities. Experiments on data from the Universal Dependency project show that phylogenetic training is beneficial to low resourced languages and well furnished languages families.
andss analysis analysis to generate insights from consumer reviews. We present a multi-domain TSA system based on augmenting a given training set with diverse weak labels from assorted domains. Extensive experiments with our approach on three evaluation datasets across different domains demonstrate the effectiveness of our solution.
a model to match word alignment hints from a phrase-based statistical machine translation model. Results show that this approach performs as well as or better than multiple state-of-the-art machine translation + monolingual retrieval systems trained on the same parallel data.
ization and extractive summarization. To mitigate these issues, this paper proposes HetFormer, a Transformer-based pre-trained model with multi-granularity sparse attentions for long-text extractive summarization. To mitigate these issues, this paper proposes HetFormer, a pre-trained model with multi-granularity sparse attentions for long-text extractive summarization.
to create texts that are challenging to distinguish from the human-written ones can be misused for generating fake news, product reviews, and even abusive content. To this end, we propose three novel types of interpretable topological features for this task based on Topological Data Analysis (TDA) which is currently understudied in the field of NLP.
, and,s, ands, ands, ands, and show that, treebank size aside, overlap between training and test graphs explain more of the observed variation than standard explanations such as the above.
, and analyzed two very different English datasets (WEBNLG and WSJ), and evaluated each algorithm using both automatic and human evaluations.Overall, the results of these evaluations suggest that rule-based systems with simple rule sets achieve on-par or better performance on both datasets compared to state-of-the-art neural REG systems.
s, and we propose a template-based input representation that significantly improves the model’s generalization capability. To further improve the model’s performance, we propose an approach based on self-training using fine-tuned BLEURT for pseudo-response selection.
a new semantic parser for English Resource Semantics. At the core of this parser is a novel neural graph rewriting system. We propose variable-in-situ logico-semantic graphs to bridge the gap between semantic graph and logical form parsing.
to satisfiability. to satisfiability. to satisfiability. to satisfiability. erroneous predictions. We use the MaxSAT semantics to model logic inference process. We adopt the MaxSAT semantics to model logic inference process. We incorporate a weighted version of MaxSAT that connects deep neural networks and a graphical model in a joint
ssssss with a statistically significant margin. To combine the benefits of both types, we propose a self-supervised contrastive learning framework to model discriminative semantic features of both in-domain intents and OOD intents from unlabeled data.
s and emotions. We conduct experiments to assess to what extent associations can be inferred from existing data in an unsupervised manner.
a novel task, Multiple TimeLine Summarization (MTLS) which extends the flexibility and versatility of Time-Line Summarization (TLS). Given any collection of time-stamped news articles, MTLS automatically discovers important yet different stories and generates a corresponding time-line for each story.
recurrence modeling, and we introduce a novel attentive recurrent network to leverage the strengths of both attention models and recurrent networks. In response to this problem, the Transformer model that is based solely on attention mechanisms, has advanced the state-of-the-art on various machine translation tasks.
, we propose a neural model EPT-X (Expression-Pointer Transformer with Explanations) which uses natural language explanations to solve an algebraic word problem. To enhance the explainability of the encoding process of a neural model, EPT-X adopts the concepts of plausibility and faithfulness.
representations of words and entities based on the bidirectional transformer. The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them.
study on abusive language towards three conversational AI systems gathered ‘in the wild’: an open-domain social bot, a rule-based chatbot, and a task-based system. To account for the complexity of the task, we take a more ‘nuanced’ approach where our ConvAI dataset reflects fine-grained notions of abuse, as well as views from multiple expert annotators.
, and solving crossword puzzles. solving crossword puzzles. the Berkeley Crossword Solver. the Berkeley Crossword Solver.-based approach for automatically solving crossword puzzles. Our system is a state-of-the-art approach for automatically solving crossword puzzles. We present the Berkeley Crossword Solver, an approach for automatically solving crossword puzzles.
and. We use linear regression for performance mining, identifying performance trends for overall classification performance and individual classifier predictions. Second, we use linear regression for performance mining, identifying performance trends for overall classification performance and individual classifier predictions.
,,,, to improve belief consistency in the overall system. First, a reasoning component – a weighted MaxSAT solver – revises beliefs that significantly clash with others. Second, a feedback component issues future queries to the PTLM using known beliefs as context. We show that, in a controlled experimental setting, these two mechanisms result in more consistent beliefs in the overall system.
and cross-lingual intent detection (ID) from spoken data. We present a systematic study on multilingual and cross-lingual intent detection (ID) from spoken data. The study leverages a new resource put forth in this work, called MInDS-14, a first training and evaluation resource for the ID task with spoken data.
s.s.s.s.. We propose a two-step method to interpret summarization model decisions.,s.s.. We propose a two-step method to interpret summarization model decisions. We first analyze the model’s behavior by ablating the full model to categorize each decoder decision into one of several generation modes. We compare these techniques based on their ability to select content
aims to find the equivalent entities between crosslingual KGs (Knowledge Graphs) which is a crucial step for integrating KGs. Recently, many GNN-based EA methods are proposed and show decent performance improvements on several public datasets. existing GNN-based EA methods inherit poor interpretability and low efficiency from neural networks.
, we design a model for incomplete utterance restoration (IUR) called JET (Joint learning token Extraction and Text generation). Different from prior studies that only work on extraction or abstraction datasets, we design a simple but effective model, working for both scenarios of IUR. Our design simulates the nature of IUR, where omitted tokens from the context contribute to restoration.
,,,, and demonstrate gains on all 4 evaluation sets. We propose a novel n-gram level retrieval approach that relies on local phrase level similarities. We evaluate our Semi-parametric NMT approach on a heterogeneous dataset composed of WMT, IWSLT, JRC-Acquis and OpenSubtitles. We demonstrate gains on all 4 evaluation sets. The Semi-parametric nature of our approach
-based dialogue modeling as machine reading comprehension task based on multiple documents. We propose MultiDoc2Dial, a new task and dataset on modeling goal-oriented dialogues grounded in multiple documents. Most previous works treat document-grounded dialogue modeling as machine reading comprehension task based on multiple documents.
boosting model performance on a language is to pre-train the model on all available supervised data from another language. However, in large-scale systems this leads to high training times and computational requirements.
of a single hidden state and the embeddings of words in the vocabulary. This single hidden state embedding cannot be close to the embeddings of all the possible next words simultaneously. Using the softmax layer, we demonstrate the importance of this limitation both theoretically and practically.
,, and,,, and conceivers. We believe the dataset and the models will be a valuable resource for a whole host of NLP applications such as fact checking and rumor detection.
, we propose an approach which trains a model that performs well over a wide range of potential test distributions. In this paper, we first show that training on text outside the test distribution can degrade test performance. In this paper, we first show that training on text outside the test distribution can degrade test performance.
and-shot learning and uncommon entities. We propose a meta-learning framework that aims at handling infrequent relations with few-shot learning and uncommon entities. We design a novel model to better extract key information from textual descriptions.
, but they struggle to identify out-of-scope queries. Our dataset and evaluation fill an important gap in the field.
,,,, and a unified classifier.,, and is faster., ands and relation extraction models setup two separated label spaces for the two sub-tasks (i.e., entity detection and relation classification). We argue that this setting may hinder the information interaction between entities and relations.
s, and augmented semantics are potential ideal resources for semantic augmentation. In this paper, we propose a neural-based approach to NER for social media texts where both local (from running text) and augmented semantics are taken into account.
to capture sentence interactions and enhance sentence representation. We conduct experiments on two datasets: a Chinese dataset and an English dataset.
the of the English BERT family and use two probing techniques to analyze how fine-tuning changes the space. We hypothesize that fine-tuning affects classification performance by increasing the distance between examples associated with different labels. We also discover an exception to the prevailing wisdom that “fine-tuning always improves performance”.
,, and semantic content. We present a novel method for measurably adjusting the semantics of text while preserving its sentiment and fluency. SMERTI can outperform baseline models on Yelp reviews, Amazon reviews, and news headlines.
, whereas distillation methods are proven for speeding up inference. We introduce a block pruning approach targeting both small and fast models. We introduce a block pruning approach targeting both small and fast models.
,,,, composition and word co-occurrence. First, we create an artificial language by modifying property in source language. Then we study the contribution of modified property through the change of cross-language transfer results on target language. First, we create an artificial language by modifying property in source language. Then we study the contribution of modified property through the change of cross-language transfer results on target language.
to improve sentence alignment quality. We propose a novel neural CRF alignment model which leverages the sequential nature of sentences in parallel documents. to evaluate and improve sentence alignment quality, we create two manually annotated sentence-aligned datasets from two commonly used text simplification corpora, Newsela and Wikipedia.
ssss that the practical learnable capacity of unsaturated RNNs obeys a similar hierarchy. We provide empirical results to support this conjecture. Experimental findings from training unsaturated networks on formal languages support this conjecture.
, we present an affect-driven dialog system, which generates emotional responses in a controlled manner using a continuous representation of emotions. The system achieve this by modeling emotions at a word and sequence level using a continuous representation of emotions. The system achieve this by modeling emotions at a word and sequence level using: (1) a vector representation of the desired emotion, (2) an affect regularizer, which penalizes neutral words, and (3) an affect sampling method
,, and,, and,, and then use them to better determine the legislative body’s vote breakdown according to demographic/ideological criteria, e.g., gender. We build a new dataset for multiple US states that interconnect multiple sources of data including bills, stakeholders, legislators, and money donors. We build a textual graph-based model to embed and analyze state bills. Our model predicts winners/losers of
, and, and. and Natural Questions datasets. a novel joint training approach for dense passage retrieval and passage reranking. retrieval and passage reranking are two key procedures in finding and ranking relevant information.,,, a novel joint training approach for dense passage retrieval and passage reranking. a novel joint training approach for dense passage retrieval and passage reranking
in neural coherence modeling. We use a basic model architecture and show significant improvements over state of the art within the same training regime. We then design a harder self-supervision objective by increasing the ratio of negative samples within a contrastive learning setup.
the topics discussed in a document. Keyphrases can be categorized into present keyphrase which explicitly appears in the source text. absent keyphrase which does not match any contiguous subsequence but is highly semantically related to the source. Select-Guide-Generate approach is proposed to deal with present and absent keyphrases generation separately with different mechanisms.
headed spans. headed spans. We propose a new method for projective dependency parsing based on headed spans. We propose a new method for projective dependency parsing based on headed spans. We decompose the score of a dependency tree into the scores of the headed spans.
a single system on the WMT’19 test set for English-German/Russian/Chinese. In particular, our English/German and English/Russian systems outperform the best single ones by over 4 BLEU points. We illustrate the capability of our scalable mining system to create high quality training sets from one language to any other.
, and it is brittle when table layouts change. We propose an equivariance learning framework, which encodes tables with a structure-aware self-attention mechanism. This prunes the full self-attention structure into an order-invariant graph attention that captures the connected graph structure of cells belonging to the same row or column.
, and we find that commonly used neural architectures exhibit different inductive biases. LSTMs display little preference with respect to word ordering, while transformers display a clear preference for some orderings over others.
, and,, and two downstream tasks — predicting speaker’s stance and expressed confidence.
a sys a sys a sys a sys a sys a sys a sys a sys a sys a sys a sys a sys a sys a sys a sys a sys a
, extracted model learns the task from the monolingual victim, but it generalizes far better than the victim to several other languages. Using a victim model, we learn the task from the monolingual victim, but it generalizes far better than the victim to several other languages.
detection a detection detection. We present a simple yet effective approach, discriminative nearest neighbor classification with deep self-attention. In this paper, we present a simple yet effective approach, discriminative nearest neighbor classification with deep self-attention.
, or lexical shortlisting, is a well-known technique to improve latency of Neural Machine Translation models. Vocabulary selection, or lexical shortlisting, is a well-known technique to improve latency of Neural Machine Translation models. this technique is a well-known technique to improve latency of Neural Machine Translation models by constraining the set of allowed output words during inference.
, and ii) release NEuRoparl-ST, a novel benchmark built from speeches annotated with NEs and terminology.
ssss, but these are brittle when faced with Out-of-Vocabulary (OOV) words.Our simple contrastive learning framework, LOVE, is a lightweight model that achieves similar or even better performances than prior competitors.
classification. Distant supervision has obtained great progress on relation classification task. However, it still suffers from noisy labeling problem.
, and suggest a method based on general rebuttal arguments to address it. Here we present a novel task of producing a critical response to a long argumentative text. Here we suggest a method based on general rebuttal arguments to address it.
, we present a new data set, called FreebaseQA, for open-domain factoid question answering (QA) tasks over structured knowledge bases, like Freebase. The data set is generated by matching trivia-type question-answer pairs with subject-predicate-object triples in Freebase. The data set is generated by matching trivia-type question-answer pairs with subject-predicate-object triples in Freebase.
,, we propose composite attention, which unites previous relative position encoding methods under convolutional framework. We conduct experiments by training BERT with composite attention, finding that convolutions consistently improve performance on multiple downstream tasks, replacing absolute position embeddings.
,, and, and, and, and an error analysis. We release our code and models for research purposes at https://github.com/sapienzaNLP/extend.
, which express multiple sentiment polarities towards different targets, resulting in overlapped feature representation. To solve this problem, we propose to utilize capsule network to construct vector-based feature representation and cluster features by an EM routing algorithm.
n-grams features, which are reminiscent of classical n-grams features. We evaluated these extracted explainable features from trained RNNs on downstream sentiment analysis tasks. found they could be used to model interesting linguistic phenomena such as negation and intensification.
ssss, which are then integrated into the summarization model. This paper attempts to address this issue by introducing a neural topic model empowered with normalizing flow to capture the global semantics of the document. This paper attempts to address this issue by introducing a neural topic model empowered with normalizing flow to capture the global semantics of the document.
pre-training without image-caption corpora. Inspiring from unsupervised pre-training, we investigate if a strong V&L representation model can be learned through unsupervised pre-training without image-caption corpora.
,,,,, and, and,, generative model has become a powerful paradigm for fast similarity search in documents retrieval. In qualitative and quantitative experiments, we show the proposed model outperforms other state-of-art methods.
, and,, and shows that non-expert annotators are successful at finding their weaknesses. We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks.
for languages not included in the initial pretraining? and 2) How can the resulting translation models effectively transfer to new domains? To answer these questions, we create a novel African news corpus covering 16 languages.
based on contexts and expert-provided rules. We propose a novel method TALLOR that bootstraps high-quality logical rules to train a neural tagger. Specifically, we introduce compound rules that are composed from simple rules to increase the precision of boundary detection.
(KBQA) (KBQA) the existing information-retrieval based methods, and remains competitive with semantic parsing based methods.
,,, and filters noisy and conflicting evidence. Our model is an LSTM-based semantic role labeler and multilingual word embeddings. Our model is an LSTM-based semantic role labeler jointly trained with a semantic role labeler.
, and we present a comprehensive taxonomy of labels for annotating misogyny in natural written language. Online misogyny, a category of online abusive language, has serious and harmful social consequences. Automated detection of online misogyny online, while imperative, poses complicated challenges to both data gathering, data annotation, and bias mitigation, as this type of data is linguistically complex and diverse
,, and 5,000 reviews in each language. Each rating constitutes 20% of the reviews in each language. For each language, there are 200,000, 5,000, and 5,000 reviews in the training, development, and test sets, respectively.
-based-down and left-corner parsing strategies as hierarchical models. In this paper, we investigated whether hierarchical structures make LMs more human-like. In computational linguistics, it has been shown that hierarchical structures make language models (LMs) more human-like.
in Indonesia: Indonesian, Javanese, and Sundanese. IndoNLG covers six tasks: summarization, question answering, chit-chat, and three different pairs of machine translation (MT) tasks. IndoNLG is the first benchmark to measure natural language generation (NLG) progress in three low-resource languages of Indonesia: Indonesian, Javanese, and Sundanese. IndoNLG covers six
,, and a polar question, indirect answer. We present BERT-based neural models to predict such categories for a question-answer pair.
sssss models. We propose neural graph matching networks, a novel sentence matching framework capable of dealing with multi-granular input information. We have developed two Chinese datasets that have outperform the state-of-the-art short text matching models.
based on Gibbs sampling. We present an approach for efficiently training and decoding hybrids of graphical models and neural networks based on Gibbs sampling. We obtain new state-of-the-art results on Dutch.
implicit abuse by separately addressing its different subtypes. For that task, we present a new focused and less biased dataset that addresses one facet of such implicit abuse. For that task, we present a new focused and less biased dataset that includes the subtype of atomic negative sentences about identity groups.
s, and the mapping between the spaces. to translate between two vector spaces given a set of aligned points arises in several applications areas of NLP. Current solutions assume that the lexicon which defines the alignment pairs is noise-free. We consider the case where the set of aligned points is allowed to contain an amount of noise, in the form of incorrect lexicon pairs.
to perform simple numerical reasoning. We add a predefined set of executable ‘programs’ which encompass simple arithmetic as well as extraction. Rather than having to learn to manipulate numbers directly, the model can pick a program and execute it.
, and establish the new state-of-the-art on standard benchmarks (ACE04, ACE05 and SciERC). In this work, we present a simple pipelined approach for entity and relation extraction aims to identify named entities and extract relations between them. In this work, we present a simple pipelined approach for entity and relation extraction, and establish the new state-of-the-art on standard benchmarks (ACE04,
, negative distillation, to keep the model away from the undesirable generic responses. Negative training is a novel negative training paradigm, called negative distillation.
, a conclusion is key to understanding an argument and, hence, to any application that processes argumentation. We develop two complementary target inference approaches: one ranks premise targets and selects the top-ranked target as the conclusion target. the other finds a new conclusion target in a learned embedding space using a triplet neural network.
generation requires the target sentence to satisfy some lexical constraints, such as containing some specific words or being the paraphrase to a given sentence. Moreover, when the search space is too large, beam-search-based methods always fail to find the constrained optimal solution.
and coreference resolution tasks. We find that UBM promises more efficient and accessible bias mitigation in LM fine-tuning.
, and,,,s, and show that magnitude pruning cannot be used to find winning lottery tickets.
from semi-structured websites has required learning an extraction model specific to a given template. In this work, we propose a solution for “zero-shot” open-domain relation extraction from webpages with a previously unseen template. Experiments show this approach provides a 31% F1 gain over a baseline for zero-shot extraction in a new subject vertical.
,s QA,, and QA research. QA research can be more effective. QA primarily descends from two branches of research: (1) Alan Turing’s investigation of machine intelligence at Manchester University and (2) Cyril Cleverdon’s comparison of library card catalog indices at Cranfield University. This position paper names and distinguishes these paradigms. Despite substantial overlap, subtle but significant distinctions exert an out
, and the inconsistent groups are obtained by sampling round-trip translations for each isolated sentence. We propose a monolingual DocRepair model to correct inconsistencies between sentence-level translations. DocRepair performs automatic post-editing on a sequence of sentence-level translations, refining translations of sentences in context of each other.
, and, the new task, and pass the knowledge to the LLL model via knowledge distillation. This is a simple but efficient method that can be easily applied to existing LLL architectures in order to mitigate the degradation. To address this issue, we present Lifelong Language Knowledge Distillation (L2KD), a simple but efficient method that can be easily applied to existing LLL architectures in order to mitigate the degradation.
LAReQA, a challenging new benchmark for language-agnostic answer retrieval from a multilingual candidate pool. LAReQA tests for “strong” cross-lingual alignment, requiring semantically related cross-language pairs to be closer in representation space than unrelated same-language pairs. LAReQA tests for “strong” cross-lingual alignment, requiring semantically related cross-language pairs to be closer in representation space than un
,,, and source values match the intuitive source-target similarity., a source valuation framework for quantifying the usefulness of the sources., an efficient source valuation framework for quantifying the usefulness of the sources. sources trained on data-rich sources to low-resource targets has been widely applied in natural language processing (NLP). However, when training a transfer model over multiple sources, not every source is equally useful
based on the underlying AMRs generated by our model, we introduce the first end-to-end AMR coreference resolution model. Besides, the document-level AMRs obtained by our model can significantly improve over the AMRs generated by a rule-based method (Liu et al., 2015) on text summarization.
, such as BERT and GPT-2, have achieved excellent performance on hard-constrained text generation. We present POINTER (PrOgressive INsertion-based TransformER), a simple yet novel insertion-based approach for hard-constrained text generation.
s,, and debiased embeddings perform almost as accurately in the triple prediction task as their non-debiased counterparts.
proposes two methods to address these two factors, respectively. The larger issue is hubness. Addressing that improves induction accuracy significantly, especially for low-frequency words.
s between vocabulary items is a key challenge in learning a new language. For example, the noun “wall” has different lexical manifestations in Spanish – “pared” refers to an indoor wall while “muro” refers to an outside wall.
for multi-domain dialogue state tracking. We show that data augmentation through synthesized data can improve the accuracy of zero-shot learning. This paper proposes new zero-short transfer learning technique for multi-domain dialogue state tracking.
,,,,,sss,,s in English as well as other languages. Their success heavily depends on the availability of a large amount of labeled data or parallel corpus. In this paper, we investigate an extreme scenario of cross-lingual sentiment classification, in which the low-resource language does not have any labels or parallel corpus.
,,, and semantic guidance for analyzing context information associated with the given entities. However, most existing studies suffer from the noise in the dependency trees, especially when they are automatically generated. In this paper, we propose a dependency-driven approach for relation extraction with attentive graph convolutional networks (A-GCN). In this approach, an attention mechanism upon graph convolutional networks is applied to different contextual words in the dependency tree obtained from an off-the
in training. We propose to use dice loss in replacement of the standard cross-entropy objective. dice loss is based on the Srensen--Dice coefficient or Tversky index. negative examples significantly outnumber positive examples, and the huge number of easy-negative examples overwhelms the training.
, and a number of baselines, including a competitive approach that uses the attention layer of a purely neural model.
, but it is difficult to enforce with off-the-shelf deep learning models. This approach improves over standard benchmarks, while also providing fine-grained control.
s (PLMs) can be interpreted as serial dual-route models, i.e., the meanings of complex words are either stored or else need be computed from the subwords. This implies that maximally meaningful input tokens should allow for the best generalization on new words.
s information allows diverse and engaging responses in dialogue response generation. Unfortunately, prior works have focused on self personas and have overlooked the value of partner personas.
ization. We use the Transformer model that uses a self-attention mechanism and has shown to be effective in capturing long-range dependencies. To learn code representation for summarization, we explore the Transformer model that uses a self-attention mechanism.